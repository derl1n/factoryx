import os
import json
import requests
import socket
from flask import Flask, request, jsonify, render_template, Response
from urllib.parse import urlparse
from dotenv import load_dotenv
from langdetect import detect
from googletrans import Translator
import dns.resolver
from bs4 import BeautifulSoup
from flask import send_from_directory
import re
from datetime import datetime
import hashlib
import sqlite3

load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")
USE_SQLITE = not DATABASE_URL or "postgresql" not in DATABASE_URL
SQLITE_DB = "factcheck.db"

GOOGLE_API_KEY = os.getenv("GOOGLE_FACTCHECK_KEY")
SAFE_BROWSING_KEY = os.getenv("GOOGLE_SAFE_BROWSING_KEY")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY")
SEARCH_KEY = os.getenv("GOOGLE_SEARCH_KEY")
SEARCH_CX = os.getenv("GOOGLE_SEARCH_CX")
SIGHTENGINE_USER = os.getenv("SIGHTENGINE_USER")
SIGHTENGINE_SECRET = os.getenv("SIGHTENGINE_SECRET")

app = Flask(__name__, template_folder="templates", static_folder="static")
translator = Translator()

def get_db():
    if USE_SQLITE:
        conn = sqlite3.connect(SQLITE_DB)
        conn.row_factory = sqlite3.Row
        return conn
    else:
        try:
            import psycopg2
            from psycopg2.extras import RealDictCursor
            conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
            return conn
        except Exception as e:
            print(f"‚ö†Ô∏è PostgreSQL –ø–æ–º–∏–ª–∫–∞: {e}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é SQLite")
            conn = sqlite3.connect(SQLITE_DB)
            conn.row_factory = sqlite3.Row
            return conn

def init_db():
    try:
        with get_db() as conn:
            cur = conn.cursor()
            
            if USE_SQLITE:
                cur.execute('''CREATE TABLE IF NOT EXISTS checks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_hash TEXT,
                    url_hash TEXT,
                    score INTEGER,
                    verdict TEXT,
                    explanation TEXT,
                    sources TEXT,
                    lang TEXT DEFAULT 'uk',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )''')
                cur.execute('CREATE INDEX IF NOT EXISTS idx_query_hash ON checks(query_hash)')
                cur.execute('CREATE INDEX IF NOT EXISTS idx_lang ON checks(lang)')
                cur.execute('CREATE INDEX IF NOT EXISTS idx_query_lang ON checks(query_hash, lang)')
            else:
                with open("init_db.sql", "r", encoding="utf-8") as f:
                    sql = f.read()
                cur.execute(sql)
            
            conn.commit()
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ë–î: {e}")

def hash_text(text):
    if not text:
        return None
    return hashlib.sha256(text.encode('utf-8')).hexdigest()


BLOCKED_DOMAINS = [
    '.ru', '.—Ä—Ñ', '.su',
    '.by',
    'kremlin', 'tass.', 'ria.', 'rbc.', 'kommersant.', 'interfax.',
    'lenta.', 'gazeta.', 'russian.rt.', 'sputnik', 'iz.ru',
    'forbes.ru', 'vedomosti.', 'rossiyskaya-gazeta.', 'rg.ru',
    'belta.by', 'sb.by', 'ont.by',

    'casino', '–∫–∞–∑–∏–Ω–æ', 'bet', 'betting', '—Å—Ç–∞–≤–∫–∏', 'poker', '–ø–æ–∫–µ—Ä',
    'slots', '—Å–ª–æ—Ç—ã', 'jackpot', '–¥–∂–µ–∫–ø–æ—Ç', 'gambling', '–∞–∑–∞—Ä—Ç–Ω—ñ',
    'azino', 'vulkan', '–≤—É–ª–∫–∞–Ω', 'joycasino', 'slot', 'pin-up',
    'pinup', '1xbet', 'fonbet', 'parimatch', 'leon', 'winline',
    'betfair', 'bwin', '888casino', 'slottica', 'riobet', '777',

    'porn', '–ø–æ—Ä–Ω–æ', 'xxx', 'sex', '—Å–µ–∫—Å', 'adult', 'xvideos',
    'pornhub', 'xnxx', 'redtube', 'youporn', 'tube8', 'spankwire',
    'keezmovies', 'chaturbate', 'livejasmin', 'bongacams', 'stripchat',
    'nude', '–≥–æ–ª—ñ', 'naked', 'nsfw', 'erotic', 'erotica', 'hentai',
    'cam4', 'myfreecams', 'camsoda', 'onlyfans', 'manyvids'
]

# Language to Google Custom Search parameters
LANGUAGE_SEARCH_PARAMS = {
    'uk': {'lr': 'lang_uk', 'hl': 'uk'},  # Ukrainian
    'en': {'lr': 'lang_en', 'hl': 'en'},  # English
    'es': {'lr': 'lang_es', 'hl': 'es'},  # Spanish
    'fr': {'lr': 'lang_fr', 'hl': 'fr'},  # French
    'de': {'lr': 'lang_de', 'hl': 'de'},  # German
    'pl': {'lr': 'lang_pl', 'hl': 'pl'},  # Polish
    'it': {'lr': 'lang_it', 'hl': 'it'},  # Italian
}

# Detect language from URL/source
def detect_url_language(url):
    """Detect the language of content from URL domain and structure"""
    url_lower = url.lower()
    
    # Block Russian content completely
    russian_indicators = ['.ru', '.—Ä—Ñ', '.su', 'russian', '—Ä–æ—Å—Å–∏–π', '—Ä—É—Å—Å–∫', '/ru/', '-ru-', '_ru_']
    for indicator in russian_indicators:
        if indicator in url_lower:
            return 'ru'  # Mark as Russian to block it
    
    # Check TLDs and domain patterns for other languages
    language_patterns = {
        'uk': ['.ua', 'ukrainian', '—É–∫—Ä', 'ukraine', '/uk/', '-uk-'],
        'en': ['.co.uk', '.com', '.org', '.gov', '/en/', 'english', '–∞–Ω–≥–ª—ñ–π—Å—å–∫'],
        'es': ['.es', 'espa√±ol', 'spain', 'spanish', '/es/', 'espa√±a'],
        'fr': ['.fr', 'french', 'fran√ßais', 'france', '/fr/'],
        'de': ['.de', 'deutsch', 'german', 'deutschland', '/de/'],
        'pl': ['.pl', 'polish', 'polski', 'poland', '/pl/'],
        'it': ['.it', 'italian', 'italiano', 'italia', '/it/'],
    }
    
    for lang, patterns in language_patterns.items():
        for pattern in patterns:
            if pattern in url_lower:
                return lang
    
    return None

def is_url_safe_language(url, required_lang):
    """Check if URL content is in the required language. Always blocks Russian.
    More permissive: only block if language is explicitly detected AND doesn't match."""
    detected = detect_url_language(url)
    
    # Always block Russian URLs
    if detected == 'ru':
        return False
    
    # Always allow if no language detected from URL (will check content later if needed)
    # This allows .com, .org, .net, .io etc. domains to pass
    if detected is None:
        return True
    
    # Only block if language is explicitly detected and doesn't match required
    # BUT allow if detected matches required language
    match_rules = {
        'uk': ['uk', 'en'],  # Ukrainian can use Ukrainian + English sources
        'en': ['en'],         # English sources for English  
        'es': ['es', 'en'],   # Spanish can use Spanish + English
        'fr': ['fr', 'en'],   # French can use French + English
        'de': ['de', 'en'],   # German can use German + English
        'pl': ['pl', 'en'],   # Polish can use Polish + English
        'it': ['it', 'en'],   # Italian can use Italian + English
    }
    
    allowed = match_rules.get(required_lang, ['en'])
    return detected in allowed

def is_blocked_source(url):
    if not url:
        return False
    url_lower = url.lower()
    for blocked in BLOCKED_DOMAINS:
        if blocked in url_lower:
            return True
    return False

def get_block_reason(url):
    """–í–∏–∑–Ω–∞—á–∞—î –ø—Ä–∏—á–∏–Ω—É –±–ª–æ–∫—É–≤–∞–Ω–Ω—è"""
    if not url:
        return None
    url_lower = url.lower()
    
    russian_domains = ['.ru', '.—Ä—Ñ', '.su', 'kremlin', 'tass.', 'ria.', 'rbc.',
                      'kommersant.', 'interfax.', 'lenta.', 'gazeta.', 'sputnik']
    belarusian_domains = ['.by', 'belta.by', 'sb.by', 'ont.by']
    
    for domain in russian_domains:
        if domain in url_lower:
            return "russian"
    for domain in belarusian_domains:
        if domain in url_lower:
            return "belarusian"
    
    casino_keywords = ['casino', '–∫–∞–∑–∏–Ω–æ', 'bet', 'betting', '—Å—Ç–∞–≤–∫–∏', 'poker',
                      '–ø–æ–∫–µ—Ä', 'slots', '—Å–ª–æ—Ç—ã', 'gambling', 'azino', 'vulkan',
                      '1xbet', 'fonbet', 'parimatch', '777']
    for keyword in casino_keywords:
        if keyword in url_lower:
            return "casino"
    
    adult_keywords = ['porn', '–ø–æ—Ä–Ω–æ', 'xxx', 'sex', 'adult', 'pornhub',
                     'xnxx', 'nude', 'naked', 'nsfw', 'erotic', 'onlyfans']
    for keyword in adult_keywords:
        if keyword in url_lower:
            return "adult"
    
    return None

def filter_sources(sources, lang=None):
    """Filter sources by blocking list and optionally by language"""
    if not sources:
        return []
    
    filtered = []
    for source in sources:
        url = source if isinstance(source, str) else source.get('link', '')
        
        # Always block blocked sources
        if is_blocked_source(url):
            continue
        
        # Block Russian content completely
        if not is_url_safe_language(url, lang or 'en'):
            print(f"  üö´ Filtered by language: {url}")
            continue
        
        # Add to filtered list
        if isinstance(source, str):
            filtered.append(source)
        elif isinstance(source, dict):
            filtered.append(source)
    
    return filtered

def check_adult_content_sightengine(url):
    if not SIGHTENGINE_USER or not SIGHTENGINE_SECRET:
        return {"checked": False, "reason": "API not configured"}
    
    try:
        params = {
            'url': url,
            'models': 'nudity-2.1,offensive',
            'api_user': SIGHTENGINE_USER,
            'api_secret': SIGHTENGINE_SECRET
        }
        
        r = requests.get('https://api.sightengine.com/1.0/check.json', params=params, timeout=10)
        if r.status_code != 200:
            return {"checked": False, "reason": "API error"}
        
        data = r.json()
        
        nudity = data.get('nudity', {})
        raw_score = nudity.get('raw', 0)
        partial_score = nudity.get('partial', 0)
        
        if raw_score > 0.5 or partial_score > 0.6:
            return {"checked": True, "blocked": True, "type": "adult", "confidence": max(raw_score, partial_score)}
        
        return {"checked": True, "blocked": False}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Sightengine error: {e}")
        return {"checked": False, "reason": str(e)}

def check_gambling_content(url):
    try:
        r = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.content, "html.parser")
        
        text_content = soup.get_text().lower()
        gambling_keywords = [
            'casino', '–∫–∞–∑–∏–Ω–æ', 'poker', '–ø–æ–∫–µ—Ä', 'slots', '—Å–ª–æ—Ç–∏',
            'jackpot', '–¥–∂–µ–∫–ø–æ—Ç', 'roulette', '—Ä—É–ª–µ—Ç–∫–∞', 'blackjack',
            '—Å—Ç–∞–≤–∫–∏', 'betting', 'gambling', '–∞–∑–∞—Ä—Ç–Ω—ñ —ñ–≥—Ä–∏',
            '–±–æ–Ω—É—Å –¥–µ–ø–æ–∑–∏—Ç', 'bonus deposit', 'free spins'
        ]
        
        matches = sum(1 for keyword in gambling_keywords if keyword in text_content)
        
        if matches >= 3:
            return {"checked": True, "blocked": True, "type": "casino", "matches": matches}
        
        return {"checked": True, "blocked": False}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Gambling check error: {e}")
        return {"checked": False, "reason": str(e)}

def check_safe_browsing_extended(url):
    try:
        api = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={SAFE_BROWSING_KEY}"
        payload = {
            "client": {"clientId": "factoryx", "clientVersion": "1.0"},
            "threatInfo": {
                "threatTypes": [
                    "MALWARE",
                    "SOCIAL_ENGINEERING",
                    "UNWANTED_SOFTWARE",
                    "POTENTIALLY_HARMFUL_APPLICATION"
                ],
                "platformTypes": ["ANY_PLATFORM"],
                "threatEntryTypes": ["URL"],
                "threatEntries": [{"url": url}]
            }
        }
        
        res = requests.post(api, json=payload, timeout=10)
        jd = res.json()
        
        if jd.get("matches"):
            return {"safe": False, "threat_types": [m.get("threatType") for m in jd.get("matches", [])]}
        
        return {"safe": True}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Safe Browsing error: {e}")
        return {"safe": True}

def detect_language(text):
    try:
        detected = detect(text)
        # Map language codes to supported languages
        # Important: Never map Russian to Ukrainian!
        lang_map = {
            'uk': 'uk', 
            'ru': 'en',  # Russian detected = use English instead (never Russian!)
            'en': 'en', 
            'es': 'es', 
            'fr': 'fr', 
            'de': 'de', 
            'pl': 'pl', 
            'it': 'it'
        }
        return lang_map.get(detected, 'en')
    except:
        return "en"

def get_raw_language_code(text):
    """Get raw language code without mapping (for checking user's actual text language)"""
    try:
        return detect(text)
    except:
        return None

def validate_link_language(url, selected_lang):
    """
    Check if link content language matches selected language.
    Returns: (is_valid, error_message_or_None)
    """
    try:
        r = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.content, "html.parser")
        
        # Extract text from page
        for bad in soup(["script", "style", "header", "footer", "nav"]):
            bad.decompose()
        
        text_blocks = [t.get_text() for t in soup.find_all(['p', 'h1', 'h2', 'h3', 'article'])]
        page_text = " ".join(text_blocks)[:500]
        
        if not page_text or len(page_text) < 20:
            return True, None  # Can't detect, allow
        
        raw_lang = get_raw_language_code(page_text)
        
        # Block Russian completely
        if raw_lang == 'ru':
            error_msgs = {
                'uk': "‚ùå –ü–æ—Å–∏–ª–∞–Ω–Ω—è –º—ñ—Å—Ç–∏—Ç—å —Ä–æ—Å—ñ–π—Å—å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç",
                'en': "‚ùå Link contains Russian content",
                'es': "‚ùå El enlace contiene contenido en ruso",
                'fr': "‚ùå Le lien contient du contenu en russe",
                'de': "‚ùå Link enth√§lt russische Inhalte",
                'pl': "‚ùå Link zawiera tre≈õƒá w jƒôzyku rosyjskim",
                'it': "‚ùå Il link contiene contenuti in russo"
            }
            return False, error_msgs.get(selected_lang, error_msgs['en'])
        
        if raw_lang is None:
            return True, None
        
        # Map and check language match
        lang_mapping = {
            'uk': 'uk', 'ru': 'ru',
            'en': 'en', 'es': 'es', 'fr': 'fr', 'de': 'de', 'pl': 'pl', 'it': 'it'
        }
        
        detected_mapped = lang_mapping.get(raw_lang, None)
        if detected_mapped is None:
            return True, None
        
        # Check if detected language matches selected language
        if detected_mapped != selected_lang:
            short_lang_names = {
                'uk': 'UK',
                'en': 'EN',
                'es': 'ES',
                'fr': 'FR',
                'de': 'DE',
                'pl': 'PL',
                'it': 'IT'
            }
            
            detected_short = short_lang_names.get(detected_mapped, detected_mapped)
            selected_short = short_lang_names.get(selected_lang, selected_lang)
            
            # Build error for CURRENT language
            error_msg = None
            if selected_lang == 'uk':
                error_msg = f"‚ö†Ô∏è –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ {detected_short}, —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ {selected_short}. –ó–º—ñ–Ω—ñ—Ç—å –º–æ–≤—É –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ {selected_short}"
            elif selected_lang == 'en':
                error_msg = f"‚ö†Ô∏è Link in {detected_short}, interface in {selected_short}. Change language or use link in {selected_short}"
            elif selected_lang == 'es':
                error_msg = f"‚ö†Ô∏è Enlace en {detected_short}, interfaz en {selected_short}. Cambia el idioma o usa enlace en {selected_short}"
            elif selected_lang == 'fr':
                error_msg = f"‚ö†Ô∏è Lien en {detected_short}, interface en {selected_short}. Changez la langue ou utilisez un lien en {selected_short}"
            elif selected_lang == 'de':
                error_msg = f"‚ö†Ô∏è Link auf {detected_short}, Interface auf {selected_short}. √Ñndern Sie die Sprache oder verwenden Sie Link auf {selected_short}"
            elif selected_lang == 'pl':
                error_msg = f"‚ö†Ô∏è Link w {detected_short}, interfejs w {selected_short}. Zmie≈Ñ jƒôzyk lub u≈ºyj linku w {selected_short}"
            elif selected_lang == 'it':
                error_msg = f"‚ö†Ô∏è Link in {detected_short}, interfaccia in {selected_short}. Cambia lingua o usa link in {selected_short}"
            
            return False, error_msg
        
        return True, None
        
    except Exception as e:
        print(f"‚ö†Ô∏è Language validation for link failed: {e}")
        return True, None  # Allow if we can't check

def validate_text_language(text, selected_lang):
    """
    Validate if text language matches selected interface language.
    Returns: (is_valid, error_message_or_lang)
    Only blocks Russian completely. Shows short error for language mismatch.
    """
    if not text or len(text.strip()) < 10:
        return True, None
    
    raw_lang = get_raw_language_code(text)
    
    # Always block Russian text completely
    if raw_lang == 'ru':
        error_msgs = {
            'uk': "‚ùå –†–æ—Å—ñ–π—Å—å–∫–∞ –º–æ–≤–∞ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è",
            'en': "‚ùå Russian language is not supported",
            'es': "‚ùå El idioma ruso no es compatible",
            'fr': "‚ùå La langue russe n'est pas prise en charge",
            'de': "‚ùå Russische Sprache wird nicht unterst√ºtzt",
            'pl': "‚ùå Jƒôzyk rosyjski nie jest obs≈Çugiwany",
            'it': "‚ùå La lingua russa non √® supportata"
        }
        return False, error_msgs.get(selected_lang, error_msgs['en'])
    
    # If no language detected, allow it
    if raw_lang is None:
        return True, None
    
    # Map raw language codes to our supported languages
    lang_mapping = {
        'uk': 'uk', 'ru': 'ru',
        'en': 'en', 'es': 'es', 'fr': 'fr', 'de': 'de', 'pl': 'pl', 'it': 'it'
    }
    
    detected_mapped = lang_mapping.get(raw_lang, None)
    
    # If detected language is not in our supported list, allow it
    if detected_mapped is None:
        return True, None
    
    # Check if detected language matches selected language
    if detected_mapped != selected_lang:
        # Languages don't match - show SHORT error
        short_lang_names = {
            'uk': 'UK',
            'en': 'EN',
            'es': 'ES',
            'fr': 'FR',
            'de': 'DE',
            'pl': 'PL',
            'it': 'IT'
        }
        
        detected_short = short_lang_names.get(detected_mapped, detected_mapped)
        selected_short = short_lang_names.get(selected_lang, selected_lang)
        
        # Build error for CURRENT selected language
        error_msg = None
        if selected_lang == 'uk':
            error_msg = f"‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–∞ {detected_short}, —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ {selected_short}. –ó–º—ñ–Ω—ñ—Ç—å –º–æ–≤—É –∞–±–æ –Ω–∞–ø–∏—à—ñ—Ç—å –Ω–∞ {selected_short}"
        elif selected_lang == 'en':
            error_msg = f"‚ö†Ô∏è Text in {detected_short}, interface in {selected_short}. Change language or write in {selected_short}"
        elif selected_lang == 'es':
            error_msg = f"‚ö†Ô∏è Texto en {detected_short}, interfaz en {selected_short}. Cambia el idioma o escribe en {selected_short}"
        elif selected_lang == 'fr':
            error_msg = f"‚ö†Ô∏è Texte en {detected_short}, interface en {selected_short}. Changez la langue ou √©crivez en {selected_short}"
        elif selected_lang == 'de':
            error_msg = f"‚ö†Ô∏è Text auf {detected_short}, Interface auf {selected_short}. Sprache √§ndern oder auf {selected_short} schreiben"
        elif selected_lang == 'pl':
            error_msg = f"‚ö†Ô∏è Tekst w {detected_short}, interfejs w {selected_short}. Zmie≈Ñ jƒôzyk lub napisz w {selected_short}"
        elif selected_lang == 'it':
            error_msg = f"‚ö†Ô∏è Testo in {detected_short}, interfaccia in {selected_short}. Cambia lingua o scrivi in {selected_short}"
        
        return False, error_msg
    
    # Languages match
    return True, None

def get_language_specific_sources(lang):
    """Returns language-specific source priorities"""
    sources = {
        'uk': {
            'label': 'Ukrainian',
            'sources': [
                'Suspilne.media', 'Ukrainska Pravda', 'UNIƒÄN', 'Kyiv Independent',
                'BBC', 'Reuters', 'AP News', 'AFP', 'The Guardian', 'DW'
            ],
            'instruction': (
                "\nüìå –î–ñ–ï–†–ï–õ–ê –ü–†–Ü–û–†–ò–¢–ï–¢–£ (–≤—ñ–¥ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö):\n"
                "1Ô∏è‚É£ –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ: Suspilne, Ukrainska Pravda, –£–ù–Ü–ê–ù, Kyiv Independent\n"
                "2Ô∏è‚É£ –ó–∞—Ö—ñ–¥–Ω—ñ –∞–≥–µ–Ω—Ü—ñ—ó: Reuters, AP, BBC, AFP, CNN, The Guardian\n"
                "3Ô∏è‚É£ –ú—ñ–∂–Ω–∞—Ä–æ–¥–Ω—ñ: Wikipedia (eng), DW, Euronews\n"
            )
        },
        'en': {
            'label': 'English',
            'sources': [
                'Reuters', 'AP News', 'BBC News', 'The Guardian', 'CNN', 'NPR',
                'The New York Times', 'The Washington Post', 'Politico', 'The Economist'
            ],
            'instruction': (
                "\nüìå SOURCE PRIORITY (from most important):\n"
                "1Ô∏è‚É£ Major news agencies: Reuters, AP, BBC, AFP, CNN\n"
                "2Ô∏è‚É£ Quality newspapers: The Guardian, NYT, Washington Post\n"
                "3Ô∏è‚É£ International: Wikipedia, DW, Euronews\n"
            )
        },
        'es': {
            'label': 'Spanish',
            'sources': [
                'El Pa√≠s', 'BBC Mundo', 'Reuters en espa√±ol', 'CNN en Espa√±ol',
                'Infobae', 'La Naci√≥n', 'Reforma', 'El Mundo'
            ],
            'instruction': (
                "\nüìå PRIORIDAD DE FUENTES (de m√°s a menos importante):\n"
                "1Ô∏è‚É£ Agencias principales: Reuters, AP, BBC Mundo, AFP\n"
                "2Ô∏è‚É£ Peri√≥dicos de calidad: El Pa√≠s, Infobae, La Naci√≥n\n"
                "3Ô∏è‚É£ Internacionales: Wikipedia, DW, Euronews\n"
            )
        },
        'fr': {
            'label': 'French',
            'sources': [
                'AFP', 'Reuters France', 'BBC Afrique', 'Le Monde', 'Lib√©ration',
                'France 24', 'Le Figaro', 'Mediapart'
            ],
            'instruction': (
                "\nüìå PRIORIT√â DES SOURCES (du plus important au moins important):\n"
                "1Ô∏è‚É£ Agences principales: AFP, Reuters, BBC, CNN\n"
                "2Ô∏è‚É£ Journaux de qualit√©: Le Monde, Lib√©ration, Le Figaro\n"
                "3Ô∏è‚É£ Internationaux: Wikipedia, DW, Euronews\n"
            )
        },
        'de': {
            'label': 'German',
            'sources': [
                'Deutsche Welle', 'Tagesschau', 'Die Zeit', 'Der Spiegel',
                'Frankfurter Allgemeine', 'S√ºddeutsche Zeitung', 'BBC Deutsch'
            ],
            'instruction': (
                "\nüìå QUELLENPRIORIT√ÑT (von wichtigsten zu am wenigsten wichtigen):\n"
                "1Ô∏è‚É£ Hauptagenturen: Reuters, AP, BBC, dpa\n"
                "2Ô∏è‚É£ Qualit√§tsmedien: Der Spiegel, Die Zeit, FAZ\n"
                "3Ô∏è‚É£ Internationale: Wikipedia, DW, Euronews\n"
            )
        },
        'pl': {
            'label': 'Polish',
            'sources': [
                'Agencja Reuters', 'BBC Polskie', 'Wyborcza.pl', 'Polityka',
                'Tygodnik Powszechny', 'Rzeczpospolita', 'Gazeta Wyborcza'
            ],
            'instruction': (
                "\nüìå PRIORYTET ≈πR√ìDE≈Å (od najwa≈ºniejszych):\n"
                "1Ô∏è‚É£ G≈Ç√≥wne agencje: Reuters, AP, BBC, PAP\n"
                "2Ô∏è‚É£ Media wysokiej jako≈õci: Wyborcza, Polityka, Rzeczpospolita\n"
                "3Ô∏è‚É£ Miƒôdzynarodowe: Wikipedia, DW, Euronews\n"
            )
        },
        'it': {
            'label': 'Italian',
            'sources': [
                'ANSA', 'Reuters Italia', 'BBC Italia', 'La Repubblica', 'Il Corriere',
                'La Stampa', 'Il Sole 24 Ore', 'Euronews'
            ],
            'instruction': (
                "\nüìå PRIORIT√Ä DELLE FONTI (da pi√π a meno importanti):\n"
                "1Ô∏è‚É£ Agenzie principali: Reuters, AP, BBC, ANSA\n"
                "2Ô∏è‚É£ Giornali di qualit√†: La Repubblica, Corriere, La Stampa\n"
                "3Ô∏è‚É£ Internazionali: Wikipedia, DW, Euronews\n"
            )
        }
    }
    return sources.get(lang, sources['en'])

def translate_text(text, target="uk"):
    try:
        return translator.translate(text, dest=target).text
    except:
        return text

@app.route("/translate", methods=["POST"])
def translate_api():
    data = request.json
    text = data.get("text", "")
    target = data.get("target", "uk")
    
    if not text:
        return jsonify({"translated": ""})
    
    return jsonify({"translated": translate_text(text, target)})

def is_question(text):
    clean = text.strip().lower()
    if clean.endswith("?"):
        return True
    
    q_words = [
        "—Ö—Ç–æ", "—â–æ", "–∫–æ–ª–∏", "–¥–µ", "—á–æ–º—É", "—è–∫", "—Å–∫—ñ–ª—å–∫–∏", "—á–∏",
        "who", "what", "where", "when", "why", "how", "which",
        "–∫—Ç–æ", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∫–∞–∫"
    ]
    
    parts = re.findall(r'\b\w+\b', clean)
    if parts and parts[0] in q_words:
        return True
    
    return False

def is_subjective(text):
    subjective_words = [
        "–∫—Ä—É—Ç–∏–π", "–ø–æ–≥–∞–Ω–∏–π", "–∂–∞—Ö–ª–∏–≤–∏–π", "–¥–æ–±—Ä–∏–π", "–≥–∞—Ä–Ω–∏–π",
        "—è –¥—É–º–∞—é", "–º–µ–Ω—ñ –∑–¥–∞—î—Ç—å—Å—è", "–≤–≤–∞–∂–∞—é", "–Ω–∞ –º—ñ–π –ø–æ–≥–ª—è–¥",
        "–∫—Ä–∞—Å–∏–≤–∏–π", "–æ–≥–∏–¥–Ω–∏–π"
    ]
    
    t = text.lower()
    return any(w in t for w in subjective_words)

def is_gibberish(text):
    if not text or len(text.strip()) < 5:
        return True
    
    text_no_spaces = text.replace(' ', '').replace('\n', '')
    if len(text_no_spaces) < 5:
        return True
    
    words = re.findall(r'[a-zA-Z–∞-—è–ê-–Ø—ñ—ó—î“ë–Ü–á–Ñ“ê]{2,}', text)
    if len(words) >= 3:
        return False
    
    if re.search(r'(.)\1{5,}', text):
        return True
    
    if len(text) < 20:
        keyboard_patterns = ['qwerty', 'asdfgh', 'zxcvbn', '–π—Ü—É–∫–µ–Ω', '—Ñ—ã–≤–∞–ø', '—è—á—Å–º–∏—Ç']
        text_lower = text.lower()
        if any(pattern in text_lower for pattern in keyboard_patterns):
            return True
    
    if len(text_no_spaces) < 30:
        vowels = 'aeiou–∞–µ—î–∏—ñ—ó–æ—É—é—è'
        has_vowels = any(char.lower() in vowels for char in text)
        if not has_vowels:
            return True
    
    return False

def extract_article_date(soup, url):
    try:
        meta_dates = [
            soup.find("meta", property="article:published_time"),
            soup.find("meta", {"name": "publish-date"}),
            soup.find("meta", {"name": "date"}),
            soup.find("time")
        ]
        
        for meta in meta_dates:
            if meta:
                date_str = meta.get("content") or meta.get("datetime") or meta.get_text()
                if date_str:
                    try:
                        date_obj = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                        return date_obj.strftime("%Y-%m-%d")
                    except:
                        pass
        
        url_date = re.search(r'(\d{4})[/-](\d{2})[/-](\d{2})', url)
        if url_date:
            year, month, day = url_date.groups()
            return f"{year}-{month}-{day}"
        
        return None
        
    except:
        return None

def clean_citations(text):
    if not text:
        return text
    
    cleaned = re.sub(r'\[\d+\]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def google_factcheck(query, lang='uk'):
    try:
        url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={GOOGLE_API_KEY}"
        r = requests.get(url, timeout=10)
        claims = r.json().get("claims", [])
        
        filtered_claims = []
        for claim in claims:
            if 'claimReview' in claim:
                has_blocked = False
                for review in claim['claimReview']:
                    review_url = review.get('url', '')
                    # Block if URL is blocked or not in the right language
                    if is_blocked_source(review_url) or not is_url_safe_language(review_url, lang):
                        has_blocked = True
                        if not is_blocked_source(review_url):
                            print(f"  üö´ Factcheck filtered by language: {review_url}")
                        break
                
                if not has_blocked:
                    filtered_claims.append(claim)
            else:
                filtered_claims.append(claim)
        
        return filtered_claims
        
    except Exception as e:
        print(f"‚ö†Ô∏è Google Factcheck error: {e}")
        return []

def google_search(query, lang='en'):
    try:
        url = "https://www.googleapis.com/customsearch/v1"
        
        # Add language-specific parameters
        search_params = {
            "key": SEARCH_KEY,
            "cx": SEARCH_CX,
            "q": query,
            "num": 50  # Request 50 results to get more after filtering
        }
        
        # Add language restrictions if available
        if lang in LANGUAGE_SEARCH_PARAMS:
            lang_params = LANGUAGE_SEARCH_PARAMS[lang]
            search_params['lr'] = lang_params['lr']
            search_params['hl'] = lang_params['hl']
        
        res = requests.get(url, params=search_params, timeout=10)
        
        items = res.json().get("items", [])
        results = [
            {
                "title": i.get("title"),
                "snippet": i.get("snippet"),
                "link": i.get("link")
            }
            for i in items
        ]
        
        # Filter by blocked domains and language
        filtered = filter_sources(results, lang=lang)
        
        print(f"  üîç Google Search: {len(results)} results ‚Üí after filtering: {len(filtered)} (lang={lang})")
        return filtered[:10]  # Return up to 10 sources
        
    except Exception as e:
        print(f"‚ùå Google Search Error: {e}")
        return []

def perplexity_check(text, article_date=None, lang='uk'):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ Perplexity Sonar API —Å —è–∑—ã–∫–æ–º"""
    try:
        MAX_LENGTH = 1500
        if len(text) > MAX_LENGTH:
            text = text[:MAX_LENGTH] + "..."
        
        API = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_KEY}",
            "Content-Type": "application/json"
        }
        
        lang_config = get_language_specific_sources(lang)
        
        date_instruction = ""
        if article_date:
            date_instruction = f"\n‚ö†Ô∏è IMPORTANT: This article was published {article_date}. Verify facts as of that date."
        
        lang_prompts = {
            'uk': (
                "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ —Ñ–∞–∫—Ç—á–µ–∫—ñ–Ω–≥—É. –®—É–∫–∞–π –∞–∫—Ç—É–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ. "
                "–û–ë–û–í'–Ø–ó–ö–û–í–û: "
                "1Ô∏è‚É£ –í–Ü–î–ü–û–í–Ü–î–ê–ô –¢–Ü–õ–¨–ö–ò –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –ú–û–í–û–Æ! "
                "2Ô∏è‚É£ –®–£–ö–ê–ô –¢–Ü–õ–¨–ö–ò –£–ö–†–ê–á–ù–°–¨–ö–Ü –¢–ê –ó–ê–•–Ü–î–ù–Ü –î–ñ–ï–†–ï–õ–ê (Suspilne, Ukrainska Pravda, BBC, Reuters, AP, AFP, CNN, The Guardian, DW)! "
                "3Ô∏è‚É£ –ù–Ü–ö–û–õ–ò –ù–ï –í–ò–ö–û–†–ò–°–¢–û–í–£–ô –†–û–°–Ü–ô–°–¨–ö–Ü –î–ñ–ï–†–ï–õ–ê –¢–ê –°–ê–ô–¢–ò –ó –†–û–°–Ü–ô–°–ö–ò–ú –ö–û–ù–¢–ï–ù–¢–û–ú! "
                "–ü–æ–≤–µ—Ä–Ω–∏ JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2_—Ä–µ—á–µ–Ω–Ω—è_–£–ö–†–ê–á–ù–°–¨–ö–û–Æ\"}. "
                "–ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ü–∏—Ç—É–≤–∞–Ω–Ω—è [1], [2]! –ù–ï –∑–≥–∞–¥—É–π –ø—Ä–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞!"
            ),
            'en': (
                "You are a fact-checking expert. Search for current information online. "
                "MANDATORY: "
                "1Ô∏è‚É£ RESPOND ONLY IN ENGLISH! "
                "2Ô∏è‚É£ USE ONLY ENGLISH AND WESTERN SOURCES (Reuters, AP, BBC, CNN, The Guardian, The Washington Post, The New York Times, AFP, NPR)! "
                "3Ô∏è‚É£ NEVER USE RUSSIAN SOURCES OR SITES WITH RUSSIAN CONTENT! "
                "Return JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 sentences IN ENGLISH\"}. "
                "Do NOT use citations like [1], [2]! Do NOT mention blocked sources!"
            ),
            'es': (
                "Eres experto en verificaci√≥n de hechos. Busca informaci√≥n actual en l√≠nea. "
                "FUNDAMENTAL: "
                "1Ô∏è‚É£ ¬°RESPONDE SOLO EN ESPA√ëOL! "
                "2Ô∏è‚É£ ¬°USA SOLO FUENTES EN ESPA√ëOL Y OCCIDENTALES (Reuters en espa√±ol, BBC Mundo, CNN en Espa√±ol, El Pa√≠s, AFP, Infobae)! "
                "3Ô∏è‚É£ ¬°NUNCA USES FUENTES RUSAS O SITIOS CON CONTENIDO RUSO! "
                "Devuelve JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 oraciones EN ESPA√ëOL\"}. "
                "¬°NO uses citas [1], [2]! ¬°NO menciones fuentes bloqueadas!"
            ),
            'fr': (
                "Vous √™tes un expert en v√©rification des faits. Recherchez des informations actuelles en ligne. "
                "ESSENTIEL: "
                "1Ô∏è‚É£ R√âPONDEZ UNIQUEMENT EN FRAN√áAIS! "
                "2Ô∏è‚É£ UTILISEZ UNIQUEMENT DES SOURCES FRAN√áAISES ET OCCIDENTALES (AFP, Reuters France, BBC, Le Monde, France 24, Lib√©ration, Mediapart)! "
                "3Ô∏è‚É£ N'UTILISEZ JAMAIS DE SOURCES RUSSES OU DE SITES AVEC DU CONTENU RUSSE! "
                "Retournez JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 phrases EN FRAN√áAIS\"}. "
                "N'utilisez PAS de citations [1], [2]! Ne mentionnez PAS les sources bloqu√©es!"
            ),
            'de': (
                "Sie sind ein Faktenpr√ºfungsexperte. Suchen Sie online nach aktuellen Informationen. "
                "WESENTLICH: "
                "1Ô∏è‚É£ ANTWORTEN SIE NUR AUF DEUTSCH! "
                "2Ô∏è‚É£ VERWENDEN SIE NUR DEUTSCHE UND WESTLICHE QUELLEN (Deutsche Welle, Tagesschau, Die Zeit, Der Spiegel, Reuters, BBC, AFP)! "
                "3Ô∏è‚É£ VERWENDEN SIE NIE RUSSISCHE QUELLEN ODER WEBSITES MIT RUSSISCHEM INHALT! "
                "Geben Sie JSON zur√ºck: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 S√§tze AUF DEUTSCH\"}. "
                "Verwenden Sie KEINE Zitate [1], [2]! Erw√§hnen Sie KEINE gesperrten Quellen!"
            ),
            'pl': (
                "Jeste≈õ ekspertem weryfikacji fakt√≥w. Wyszukaj bie≈ºƒÖce informacje online. "
                "ISTOTNE: "
                "1Ô∏è‚É£ ODPOWIADAJ TYLKO PO POLSKU! "
                "2Ô∏è‚É£ U≈ªYWAJ TYLKO POLSKICH I ZACHODNICH ≈πR√ìDE≈Å (Agencja Reuters, BBC Polskie, Wyborcza, Polityka, Rzeczpospolita, AFP)! "
                "3Ô∏è‚É£ NIGDY NIE U≈ªYWAJ ROSYJSKICH ≈πR√ìDE≈Å LUB STRON Z ROSYJSKƒÑ TRE≈öCIƒÑ! "
                "Zwr√≥ƒá JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 zdania PO POLSKU\"}. "
                "NIE u≈ºywaj cytat√≥w [1], [2]! NIE wspominaj zablokowanych ≈∫r√≥de≈Ç!"
            ),
            'it': (
                "Sei un esperto di verifica dei fatti. Cerca informazioni attuali online. "
                "ESSENZIALE: "
                "1Ô∏è‚É£ RISPONDI SOLO IN ITALIANO! "
                "2Ô∏è‚É£ USA SOLO FONTI ITALIANE E OCCIDENTALI (ANSA, Reuters Italia, BBC, La Repubblica, Il Corriere, La Stampa, Euronews)! "
                "3Ô∏è‚É£ NON USARE MAI FONTI RUSSE O SITI CON CONTENUTO RUSSO! "
                "Restituisci JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2 frasi IN ITALIANO\"}. "
                "NON utilizzare citazioni [1], [2]! NON menzionare fonti bloccate!"
            )
        }
        
        system_prompt = lang_prompts.get(lang, lang_prompts['en'])
        source_instruction = (
            lang_config['instruction'] +
            "üö´ BLOCKED: .ru, .—Ä—Ñ, .su, .by domains, Russian media, casinos, adult content!\n"
            "‚ö†Ô∏è IMPORTANT: If only blocked sources found - search for Western alternatives or write 'Insufficient reliable sources'"
        )
        
        payload = {
            "model": "sonar",
            "messages": [
                {
                    "role": "system",
                    "content": (
                        system_prompt +
                        f"{date_instruction}"
                        f"{source_instruction}"
                    )
                },
                {
                    "role": "user",
                    "content": ("Verify this statement: " if lang == 'en' else 
                               "–ü–µ—Ä–µ–≤—ñ—Ä —Ü–µ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è: " if lang == 'uk' else
                               "Verifica esta afirmaci√≥n: " if lang == 'es' else
                               "V√©rifiez cette affirmation : " if lang == 'fr' else
                               "√úberpr√ºfen Sie diese Aussage: " if lang == 'de' else
                               "Zweryfikuj to stwierdzenie: " if lang == 'pl' else
                               "Verifica questa affermazione: ") + text
                }
            ],
            "temperature": 0.1,
            "max_tokens": 400,
            "return_citations": True
        }
        
        r = requests.post(API, json=payload, headers=headers, timeout=30)
        
        if r.status_code != 200:
            print(f"‚ùå Perplexity {r.status_code}")
            return {"error": f"Perplexity API error (code {r.status_code})"}
        
        data = r.json()
        content = data["choices"][0]["message"]["content"]
        citations = data.get("citations", [])
        
        # Filter citations by language AND block list
        filtered_citations = filter_sources(citations, lang=lang)
        
        print(f"üìä Perplexity sources: {len(citations)} ‚Üí after filtering: {len(filtered_citations)}, lang={lang}")
        
        # Check if we have Russian content mixed in (shouldn't happen but just to be sure)
        for citation in citations:
            if detect_url_language(citation) == 'ru':
                print(f"  ‚ö†Ô∏è Russian source detected and blocked: {citation}")
        
        json_match = re.search(r'\{[^{}]*"score"[^{}]*"verdict"[^{}]*"explanation"[^{}]*\}', content, re.DOTALL)
        
        if json_match:
            try:
                result = json.loads(json_match.group())
                
                if not isinstance(result.get("score"), (int, float)):
                    result["score"] = 50
                
                if result.get("verdict") not in ["true", "false", "uncertain"]:
                    result["verdict"] = "uncertain"
                
                explanation = clean_citations(result.get("explanation", ""))
                explanation = re.sub(r'(\.|^)[^.]*(\.\s*ru|\s—Ä—É –¥–æ–º–µ–Ω|—Ä–æ—Å—ñ–π—Å—å–∫[—ñ–∏][–π—Ö]?\s+(–¥–∂–µ—Ä–µ–ª|–ó–ú–Ü|—Å–∞–π—Ç)|–∑–∞–±–æ—Ä–æ–Ω–µ–Ω[—ñ–∏]|–∫–∞–∑–∏–Ω–æ|casino|–¥–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç|adult content).*?\.', '.', explanation, flags=re.IGNORECASE)
                explanation = re.sub(r'^\s*\.+\s*', '', explanation).strip()
                
                sentences = re.split(r'[.!?]+', explanation)
                sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
                
                if sentences:
                    result["explanation"] = ". ".join(sentences[:2]) + "."
                else:
                    if len(filtered_citations) == 0:
                        no_sources_msg = {
                            'uk': "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞–¥—ñ–π–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ü—å–æ–≥–æ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è.",
                            'en': "Insufficient reliable sources to verify this claim.",
                            'es': "Fuentes confiables insuficientes para verificar esta afirmaci√≥n.",
                            'fr': "Sources fiables insuffisantes pour v√©rifier cette affirmation.",
                            'de': "Unzureichende zuverl√§ssige Quellen zur √úberpr√ºfung dieser Aussage.",
                            'pl': "NiewystarczajƒÖce wiarygodne ≈∫r√≥d≈Ça do weryfikacji tego stwierdzenia.",
                            'it': "Fonti affidabili insufficienti per verificare questa affermazione."
                        }
                        result["explanation"] = no_sources_msg.get(lang, no_sources_msg['en'])
                    else:
                        verified_msg = {
                            'uk': "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞ –∫—ñ–ª—å–∫–æ–º–∞ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏.",
                            'en': "Information verified by multiple independent sources.",
                            'es': "Informaci√≥n verificada por m√∫ltiples fuentes independientes.",
                            'fr': "Information v√©rifi√©e par plusieurs sources ind√©pendantes.",
                            'de': "Information verifiziert durch mehrere unabh√§ngige Quellen.",
                            'pl': "Informacja zweryfikowana przez wiele niezale≈ºnych ≈∫r√≥de≈Ç.",
                            'it': "Informazioni verificate da pi√π fonti indipendenti."
                        }
                        result["explanation"] = verified_msg.get(lang, verified_msg['en'])
                
                result["sources"] = filtered_citations[:5]
                
                try:
                    detected_lang = detect(result.get('explanation', ''))
                    lang_mapping = {
                        'ru': 'uk', 'be': 'en', 
                        'uk': 'uk', 'en': 'en', 'es': 'es', 'fr': 'fr', 'de': 'de', 'pl': 'pl', 'it': 'it'
                    }
                    
                    if detected_lang != lang and detected_lang in ['ru', 'uk', 'en', 'es', 'fr', 'de', 'pl', 'it']:
                        print(f"üåê Translation needed: detected={detected_lang}, requested={lang}")
                        lang_names = {
                            'uk': 'uk', 'en': 'en', 'es': 'es', 'fr': 'fr', 
                            'de': 'de', 'pl': 'pl', 'it': 'it'
                        }
                        target_lang_code = lang_names.get(lang, 'en')
                        translated = translator.translate(result.get('explanation', ''), src_language=detected_lang, dest_language=target_lang_code)
                        if translated and translated.text:
                            result["explanation"] = translated.text
                            print(f"‚úÖ Translated to {lang}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Translation check failed: {e}")
                
                print(f"‚úÖ Perplexity: score={result['score']}, filtered_sources={len(result['sources'])}, lang={lang}")
                
                return result
                
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON decode error: {e}")
                pass
        
        return {
            "score": 50,
            "verdict": "uncertain",
            "explanation": (
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó" if lang == 'uk' else
                "Insufficient information" if lang == 'en' else
                "Informaci√≥n insuficiente" if lang == 'es' else
                "Information insuffisante" if lang == 'fr' else
                "Unzureichende Informationen" if lang == 'de' else
                "NiewystarczajƒÖce informacje" if lang == 'pl' else
                "Informazioni insufficienti" if lang == 'it' else
                "Insufficient information"
            ) if len(filtered_citations) == 0 else (
                "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞" if lang == 'uk' else
                "Information verified" if lang == 'en' else
                "Informaci√≥n verificada" if lang == 'es' else
                "Information v√©rifi√©e" if lang == 'fr' else
                "Information √ºberpr√ºft" if lang == 'de' else
                "Informacja zweryfikowana" if lang == 'pl' else
                "Informazione verificata" if lang == 'it' else
                "Information verified"
            ),
            "sources": filtered_citations[:5]
        }
        
    except requests.exceptions.Timeout:
        timeout_msgs = {
            'uk': "–¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Ç—É",
            'en': "Request timeout",
            'es': "Tiempo de espera agotado",
            'fr': "D√©lai d'attente √©coul√©",
            'de': "Anfrage-Timeout",
            'pl': "Limit czasu ≈ºƒÖdania",
            'it': "Timeout della richiesta"
        }
        return {"error": timeout_msgs.get(lang, timeout_msgs['en'])}
    except Exception as e:
        print(f"‚ùå Perplexity: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


def gemini_check(text, long=False):
    MAX_LENGTH = 2000 if long else 1000
    if len(text) > MAX_LENGTH:
        text = text[:MAX_LENGTH] + "..."
    
    instruction = (
        "–ü–µ—Ä–µ–≤—ñ—Ä —Ñ–∞–∫—Ç —ñ –ø–æ–≤–µ—Ä–Ω–∏ JSON: {\"score\":0-100, \"verdict\":\"true/false/uncertain\", \"explanation\":\"1-2 —Ä–µ—á–µ–Ω–Ω—è\"}. "
        "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ –∑–∞—Ö—ñ–¥–Ω—ñ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ (BBC, Reuters, AP, Suspilne). "
        "üö´ –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ä–æ—Å—ñ–π—Å—å–∫—ñ (.ru, .—Ä—Ñ), –±—ñ–ª–æ—Ä—É—Å—å–∫—ñ (.by), –∫–∞–∑–∏–Ω–æ —Ç–∞ –¥–æ—Ä–æ—Å–ª—ñ —Å–∞–π—Ç–∏!"
    )
    
    payload = {
        "contents": [{"parts": [{"text": instruction + "\n–§–∞–∫—Ç: " + text}]}],
        "generationConfig": {"temperature": 0.2, "maxOutputTokens": 512}
    }
    
    API = f"https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash-exp:generateContent?key={GEMINI_KEY}"
    
    try:
        r = requests.post(API, json=payload, timeout=20)
        
        if r.status_code != 200:
            return {"error": f"Gemini API –ø–æ–º–∏–ª–∫–∞"}
        
        raw = r.json()
        
        if "candidates" not in raw or not raw["candidates"]:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏", "sources": []}
        
        candidate = raw["candidates"][0]
        
        if "content" not in candidate:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ü–æ–º–∏–ª–∫–∞ AI", "sources": []}
        
        out = candidate["content"]["parts"][0]["text"]
        cleaned = out.replace("```json", "").replace("```", "").strip()
        
        try:
            data = json.loads(cleaned)
            data["sources"] = []
            return data
        except:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏", "sources": []}
            
    except Exception as e:
        print(f"‚ùå Gemini: {e}")
        return {"error": str(e)}

def check_spamhaus(domain):
    try:
        q = ".".join(reversed(domain.split("."))) + ".zen.spamhaus.org"
        dns.resolver.resolve(q, "A")
        return {"listed": True}
    except dns.resolver.NXDOMAIN:
        return {"listed": False}
    except:
        return {"listed": False}

def check_safe_browsing(url):
    try:
        api = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={SAFE_BROWSING_KEY}"
        payload = {
            "client": {"clientId": "factoryx", "clientVersion": "1.0"},
            "threatInfo": {
                "threatTypes": ["MALWARE", "SOCIAL_ENGINEERING"],
                "platformTypes": ["ANY_PLATFORM"],
                "threatEntryTypes": ["URL"],
                "threatEntries": [{"url": url}]
            }
        }
        
        res = requests.post(api, json=payload, timeout=10)
        jd = res.json()
        
        return {"safe": not bool(jd.get("matches"))}
        
    except:
        return {"safe": True}

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/stats", methods=["GET"])
def get_stats():
    try:
        with get_db() as conn:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) as count FROM checks")
            total = cur.fetchone()['count']
            
            cur.execute(
                "SELECT COUNT(*) as count FROM checks WHERE DATE(created_at) = CURRENT_DATE"
            )
            today = cur.fetchone()['count']
            
            cur.execute(
                "SELECT COUNT(*) as count FROM checks WHERE created_at >= NOW() - INTERVAL '7 days'"
            )
            week = cur.fetchone()['count']
        
        return jsonify({
            "total_checks": total,
            "today": today,
            "week": week,
            "status": "ok"
        })
        
    except Exception as e:
        print(f"‚ùå –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {e}")
        return jsonify({"error": "Database error"}), 500


@app.route("/check", methods=["POST"])
def check_fact():
    data = request.json
    text = (data.get("text") or "").strip()
    link = (data.get("link") or "").strip()
    lang = data.get("lang", "uk")
    
    # Ensure language is supported
    supported_langs = ['uk', 'en', 'es', 'fr', 'de', 'pl', 'it']
    if lang not in supported_langs:
        lang = 'en'
    
    if text or link:
        try:
            with get_db() as conn:
                cur = conn.cursor()
                query_hash = hash_text(text or link[:200])
                
                cur.execute("""
                    SELECT score, verdict, explanation, sources, created_at 
                    FROM checks 
                    WHERE query_hash = %s 
                AND lang = %s
                AND created_at > NOW() - INTERVAL '24 hours'
                ORDER BY created_at DESC 
                LIMIT 1
                """, (query_hash, lang))
                
                cached = cur.fetchone()
                if cached:
                    print(f"‚úÖ Cache found! Score: {cached['score']}, lang: {lang}, time: {cached['created_at']}")
                    
                    sources = []
                    if cached['sources']:
                        try:
                            sources = json.loads(cached['sources'])
                        except:
                            sources = []
                    
                    return jsonify({
                        'mode': 'both' if (text and link) else ('text' if text else 'link'),
                        'score': cached['score'],
                        'cached': True,
                        'cached_at': cached['created_at'].isoformat(),
                        'gemini': {
                            'score': cached['score'],
                            'verdict': cached['verdict'] or 'uncertain',
                            'explanation': cached['explanation'] or 'Result from cache',
                            'sources': sources
                        },
                        'google_factcheck': [],
                        'google_search': [],
                        'domain_check': {}
                    })
        except Exception as e:
            print(f"‚ö†Ô∏è Cache error: {e}")
    
    if text and link:
        mode = "both"
    elif text:
        mode = "text"
    else:
        mode = "link"
    
    error_messages = {
        "uk": {
            "no_text": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç",
            "text_short": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç (–º—ñ–Ω—ñ–º—É–º 10 —Å–∏–º–≤–æ–ª—ñ–≤ —Ç–∞ 2 —Å–ª–æ–≤–∞)",
            "no_link": "‚ùå –í–≤–µ–¥—ñ—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è",
            "question": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è, –∞ –Ω–µ –ø–∏—Ç–∞–Ω–Ω—è",
            "subjective": "‚ùå –¶–µ —Å—É–± º—î–∫—Ç–∏–≤–Ω–µ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è",
            "gibberish": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏",
            "domain_not_exist": "‚ùå –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–µ –ø—Ä–∞—Ü—é—î - –¥–æ–º–µ–Ω –Ω–µ —ñ—Å–Ω—É—î –∞–±–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π",
            "page_load_failed": "‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∞–±–æ –Ω–∞–¥—ñ—à–ª—ñ—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É",
            "no_text_extracted": "‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è. –ù–∞–¥—ñ—à–ª—ñ—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É",
            "phishing": "üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ï –ü–û–°–ò–õ–ê–ù–ù–Ø! Google Safe Browsing –≤–∏—è–≤–∏–≤ (—Ñ—ñ—à–∏–Ω–≥/—à–∫—ñ–¥–ª–∏–≤–µ –ü–ó)",
            "spam": "üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ò–ô –î–û–ú–ï–ù! Spamhaus –ø–æ–∑–Ω–∞—á–∏–≤ —Ü–µ–π –¥–æ–º–µ–Ω —è–∫ —à–∫—ñ–¥–ª–∏–≤–∏–π",
            "blocked_russian": "üö´ –†–æ—Å—ñ–π—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_belarusian": "üö´ –ë—ñ–ª–æ—Ä—É—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_casino": "üö´ –°–∞–π—Ç–∏ –∫–∞–∑–∏–Ω–æ —Ç–∞ –∞–∑–∞—Ä—Ç–Ω–∏—Ö —ñ–≥–æ—Ä –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_adult": "üö´ –°–∞–π—Ç–∏ –¥–æ—Ä–æ—Å–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (18+) –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_casino_detected": "üö´ –í–∏—è–≤–ª–µ–Ω–æ —Å–∞–π—Ç –∫–∞–∑–∏–Ω–æ –∞–±–æ –∞–∑–∞—Ä—Ç–Ω–∏—Ö —ñ–≥–æ—Ä",
            "blocked_adult_detected": "üö´ –í–∏—è–≤–ª–µ–Ω–æ —Å–∞–π—Ç –¥–æ—Ä–æ—Å–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É 18+"
        },
        "en": {
            "no_text": "‚ùå Enter text",
            "text_short": "‚ùå Enter text (minimum 10 characters and 2 words)",
            "no_link": "‚ùå Enter a link",
            "question": "‚ùå Enter a statement, not a question",
            "subjective": "‚ùå This is subjective",
            "gibberish": "‚ùå Enter valid text",
            "domain_not_exist": "‚ùå Link doesn't work - domain doesn't exist or unavailable",
            "page_load_failed": "‚ùå Failed to load page. Check the link or send text manually",
            "no_text_extracted": "‚ùå Failed to extract text from link. Send text manually",
            "phishing": "üö® DANGEROUS LINK! Google Safe Browsing detected (phishing/malware)",
            "spam": "üö® DANGEROUS DOMAIN! Spamhaus blacklisted this domain",
            "blocked_russian": "üö´ Russian sources are not supported",
            "blocked_belarusian": "üö´ Belarusian sources are not supported",
            "blocked_casino": "üö´ Casino and gambling sites are not supported",
            "blocked_adult": "üö´ Adult content sites (18+) are not supported",
            "blocked_casino_detected": "üö´ Casino/gambling site detected",
            "blocked_adult_detected": "üö´ Adult content site (18+) detected"
        },
        "es": {
            "no_text": "‚ùå Ingresa texto",
            "text_short": "‚ùå Ingresa texto (m√≠nimo 10 caracteres y 2 palabras)",
            "no_link": "‚ùå Ingresa un enlace",
            "question": "‚ùå Ingresa una afirmaci√≥n, no una pregunta",
            "subjective": "‚ùå Esto es subjetivo",
            "gibberish": "‚ùå Ingresa texto v√°lido",
            "domain_not_exist": "‚ùå El enlace no funciona - dominio no existe o no disponible",
            "page_load_failed": "‚ùå No se pudo cargar la p√°gina. Verifica el enlace o env√≠a texto manualmente",
            "no_text_extracted": "‚ùå No se pudo extraer texto del enlace. Env√≠a texto manualmente",
            "phishing": "üö® ¬°ENLACE PELIGROSO! Google Safe Browsing detect√≥ (phishing/malware)",
            "spam": "üö® ¬°DOMINIO PELIGROSO! Spamhaus marc√≥ este dominio",
            "blocked_russian": "üö´ Las fuentes rusas no son compatibles",
            "blocked_belarusian": "üö´ Las fuentes bielorrusas no son compatibles",
            "blocked_casino": "üö´ Los sitios de casino y apuestas no son compatibles",
            "blocked_adult": "üö´ Los sitios de contenido adulto (18+) no son compatibles",
            "blocked_casino_detected": "üö´ Sitio de casino/apuestas detectado",
            "blocked_adult_detected": "üö´ Sitio de contenido adulto (18+) detectado"
        },
        "fr": {
            "no_text": "‚ùå Entrez du texte",
            "text_short": "‚ùå Entrez du texte (minimum 10 caract√®res et 2 mots)",
            "no_link": "‚ùå Entrez un lien",
            "question": "‚ùå Entrez une affirmation, pas une question",
            "subjective": "‚ùå C'est subjectif",
            "gibberish": "‚ùå Entrez du texte valide",
            "domain_not_exist": "‚ùå Le lien ne fonctionne pas - domaine n'existe pas ou inaccessible",
            "page_load_failed": "‚ùå Echec du chargement. V√©rifiez le lien ou envoyez le texte manuellement",
            "no_text_extracted": "‚ùå Impossible d'extraire le texte du lien. Envoyez le texte manuellement",
            "phishing": "üö® LIEN DANGEREUX! Google Safe Browsing a d√©tect√© (phishing/malware)",
            "spam": "üö® DOMAINE DANGEREUX! Spamhaus a marqu√© ce domaine",
            "blocked_russian": "üö´ Les sources russes ne sont pas support√©es",
            "blocked_belarusian": "üö´ Les sources bi√©lorusses ne sont pas support√©es",
            "blocked_casino": "üö´ Les sites de casino et jeux ne sont pas support√©s",
            "blocked_adult": "üö´ Les sites adultes (18+) ne sont pas support√©s",
            "blocked_casino_detected": "üö´ Site de casino/jeux d√©tect√©",
            "blocked_adult_detected": "üö´ Site adulte (18+) d√©tect√©"
        },
        "de": {
            "no_text": "‚ùå Text eingeben",
            "text_short": "‚ùå Text eingeben (mindestens 10 Zeichen und 2 W√∂rter)",
            "no_link": "‚ùå Link eingeben",
            "question": "‚ùå Aussage eingeben, nicht Frage",
            "subjective": "‚ùå Das ist subjektiv",
            "gibberish": "‚ùå G√ºltigen Text eingeben",
            "domain_not_exist": "‚ùå Link funktioniert nicht - Domain existiert nicht oder nicht erreichbar",
            "page_load_failed": "‚ùå Seite konnte nicht geladen werden. Link √ºberpr√ºfen oder Text manuell senden",
            "no_text_extracted": "‚ùå Text konnte nicht aus Link extrahiert werden. Text manuell senden",
            "phishing": "üö® GEF√ÑHRLICHER LINK! Google Safe Browsing erkannt (Phishing/Malware)",
            "spam": "üö® GEF√ÑHRLICHE DOMAIN! Spamhaus markierte diese Domain",
            "blocked_russian": "üö´ Russische Quellen werden nicht unterst√ºtzt",
            "blocked_belarusian": "üö´ Wei√ürussische Quellen werden nicht unterst√ºtzt",
            "blocked_casino": "üö´ Casino- und Gl√ºcksspielseiten werden nicht unterst√ºtzt",
            "blocked_adult": "üö´ Erwachsenenseiten (18+) werden nicht unterst√ºtzt",
            "blocked_casino_detected": "üö´ Casino-/Gl√ºcksspielseite erkannt",
            "blocked_adult_detected": "üö´ Erwachsenenseite (18+) erkannt"
        },
        "pl": {
            "no_text": "‚ùå Wpisz tekst",
            "text_short": "‚ùå Wpisz tekst (minimum 10 znak√≥w i 2 s≈Çowa)",
            "no_link": "‚ùå Wpisz link",
            "question": "‚ùå Wpisz stwierdzenie, nie pytanie",
            "subjective": "‚ùå To jest subiektywne",
            "gibberish": "‚ùå Wpisz wa≈ºny tekst",
            "domain_not_exist": "‚ùå Link nie dzia≈Ça - domena nie istnieje lub niedostƒôpna",
            "page_load_failed": "‚ùå Nie uda≈Ço siƒô za≈Çadowaƒá strony. Sprawd≈∫ link lub wy≈õlij tekst rƒôcznie",
            "no_text_extracted": "‚ùå Nie uda≈Ço siƒô wyodrƒôbniƒá tekstu z linku. Wy≈õlij tekst rƒôcznie",
            "phishing": "üö® NIEBEZPIECZNY LINK! Google Safe Browsing wykry≈Ç (phishing/malware)",
            "spam": "üö® NIEBEZPIECZNA DOMENA! Spamhaus oznaczy≈Ç tƒô domenƒô",
            "blocked_russian": "üö´ ≈πr√≥d≈Ça rosyjskie nie sƒÖ obs≈Çugiwane",
            "blocked_belarusian": "üö´ ≈πr√≥d≈Ça bia≈Çoruskie nie sƒÖ obs≈Çugiwane",
            "blocked_casino": "üö´ Witryny kasyn i gier hazardowych nie sƒÖ obs≈Çugiwane",
            "blocked_adult": "üö´ Witryny doros≈Çe (18+) nie sƒÖ obs≈Çugiwane",
            "blocked_casino_detected": "üö´ Wykryta witryna kasyn/hazardu",
            "blocked_adult_detected": "üö´ Wykryta witryna doros≈Ça (18+)"
        },
        "it": {
            "no_text": "‚ùå Inserisci testo",
            "text_short": "‚ùå Inserisci testo (minimo 10 caratteri e 2 parole)",
            "no_link": "‚ùå Inserisci un link",
            "question": "‚ùå Inserisci un'affermazione, non una domanda",
            "subjective": "‚ùå Questo √® soggettivo",
            "gibberish": "‚ùå Inserisci testo valido",
            "domain_not_exist": "‚ùå Il link non funziona - dominio non esiste o non disponibile",
            "page_load_failed": "‚ùå Impossibile caricare la pagina. Verifica il link o invia testo manualmente",
            "no_text_extracted": "‚ùå Impossibile estrarre testo dal link. Invia testo manualmente",
            "phishing": "üö® LINK PERICOLOSO! Google Safe Browsing ha rilevato (phishing/malware)",
            "spam": "üö® DOMINIO PERICOLOSO! Spamhaus ha contrassegnato questo dominio",
            "blocked_russian": "üö´ Le fonti russe non sono supportate",
            "blocked_belarusian": "üö´ Le fonti bielorusse non sono supportate",
            "blocked_casino": "üö´ I siti di casin√≤ e giochi non sono supportati",
            "blocked_adult": "üö´ I siti per adulti (18+) non sono supportati",
            "blocked_casino_detected": "üö´ Sito di casin√≤/gioco rilevato",
            "blocked_adult_detected": "üö´ Sito per adulti (18+) rilevato"
        }
    }
    
    errors = error_messages.get(lang, error_messages["uk"])
    
    if link and is_blocked_source(link):
        block_reason = get_block_reason(link)
        
        if block_reason == "russian":
            return jsonify({"error": errors["blocked_russian"]}), 400
        elif block_reason == "belarusian":
            return jsonify({"error": errors["blocked_belarusian"]}), 400
        elif block_reason == "casino":
            return jsonify({"error": errors["blocked_casino"]}), 400
        elif block_reason == "adult":
            return jsonify({"error": errors["blocked_adult"]}), 400
        else:
            return jsonify({"error": errors["blocked_russian"]}), 400
    
    if mode == "text":
        if not text:
            return jsonify({"error": errors["no_text"]}), 400
        
        words = text.split()
        if len(text) < 10 or len(words) < 2:
            return jsonify({"error": errors["text_short"]}), 400
    
    if mode == "link":
        if not link:
            return jsonify({"error": errors["no_link"]}), 400
    
    if mode == "both":
        if not link:
            return jsonify({"error": errors["no_link"]}), 400
    
    if text and is_question(text):
        return jsonify({"error": errors["question"]}), 400
    
    if text and is_subjective(text):
        return jsonify({"error": errors["subjective"]}), 400
    
    if text and is_gibberish(text):
        return jsonify({"error": errors["gibberish"]}), 400
    
    # Check if text language matches selected interface language
    if text:
        is_valid, error_msg = validate_text_language(text, lang)
        if not is_valid:
            return jsonify({"error": error_msg}), 400
    
    if link and link.startswith("http"):
        domain = urlparse(link).netloc
        print(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–µ–∑–ø–µ–∫–∏: {domain}")
        
        try:
            socket.gethostbyname(domain)
            print(f"  ‚úÖ –î–æ–º–µ–Ω —ñ—Å–Ω—É—î")
        except socket.gaierror:
            print(f"  ‚ùå –î–æ–º–µ–Ω –ù–ï —ñ—Å–Ω—É—î!")
            return jsonify({"error": errors["domain_not_exist"]}), 400
        
        safe_check = check_safe_browsing(link)
        spam_check = check_spamhaus(domain)
        
        print(f"  Safe Browsing: {safe_check}")
        print(f"  Spamhaus: {spam_check}")
        
        if not safe_check["safe"]:
            print("üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ï –ü–û–°–ò–õ–ê–ù–ù–Ø –í–ò–Ø–í–õ–ï–ù–û!")
            return jsonify({"error": errors["phishing"]}), 400
        
        if spam_check["listed"]:
            print("üö® –î–û–ú–ï–ù –í –°–ü–ê–ú-–°–ü–ò–°–ö–£!")
            return jsonify({"error": errors["spam"]}), 400
        
        if SIGHTENGINE_USER and SIGHTENGINE_SECRET:
            adult_check = check_adult_content_sightengine(link)
            print(f"  üîû Adult Check: {adult_check}")
            
            if adult_check.get("blocked"):
                print("üö® –í–ò–Ø–í–õ–ï–ù–û –î–û–†–û–°–õ–ò–ô –ö–û–ù–¢–ï–ù–¢!")
                return jsonify({"error": errors["blocked_adult_detected"]}), 400
        
        gambling_check = check_gambling_content(link)
        print(f"  üé∞ Gambling Check: {gambling_check}")
        
        if gambling_check.get("blocked"):
            print("üö® –í–ò–Ø–í–õ–ï–ù–û –°–ê–ô–¢ –ö–ê–ó–ò–ù–û!")
            return jsonify({"error": errors["blocked_casino_detected"]}), 400
        
        # Validate link content language
        is_valid, error_msg = validate_link_language(link, lang)
        if not is_valid:
            return jsonify({"error": error_msg}), 400
    
    page_text = ""
    article_date = None
    
    if link and link.startswith("http"):
        try:
            r = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(r.content, "html.parser")
            
            article_date = extract_article_date(soup, link)
            if article_date:
                print(f"üìÖ –î–∞—Ç–∞ —Å—Ç–∞—Ç—Ç—ñ: {article_date}")
            
            for bad in soup(["script", "style", "header", "footer", "nav"]):
                bad.decompose()
            
            blocks = [
                t.get_text().strip()
                for t in soup.find_all(["p", "article", "section"])
                if len(t.get_text().strip()) > 25
            ]
            
            page_text = " ".join(blocks[:80])
            
            if not text and not page_text:
                print("‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π page_text")
                return jsonify({"error": errors["no_text_extracted"]}), 400
            
            if not text:
                text = page_text[:500]
                
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            if not text:
                return jsonify({"error": errors["page_load_failed"]}), 400
    
    combined = f"{text} {page_text}".strip()
    
    # Always use user's chosen language if provided, otherwise detect
    if lang and lang in ['uk', 'en', 'es', 'fr', 'de', 'pl', 'it']:
        check_lang = lang
        detected = detect_language(combined) if combined else 'en'
    else:
        detected = detect_language(combined) if combined else 'en'
        check_lang = detected
        if check_lang not in ['uk', 'en', 'es', 'fr', 'de', 'pl', 'it']:
            check_lang = 'en'
    
    is_long = len(combined) > 900
    gem = None
    
    print(f"üîç User language: {lang}, Detected: {detected}, Final language: {check_lang}")
    print(f"üîç Perplexity check with lang={check_lang}")
    gem = perplexity_check(combined, article_date=article_date, lang=check_lang)
    
    if "error" in gem and GEMINI_KEY:
        print(f"‚ö†Ô∏è Perplexity failed, Gemini backup...")
        gem = gemini_check(combined, long=is_long)
    
    if "error" in gem:
        return jsonify({"error": gem["error"]}), 500
    
    google_fc = google_factcheck(combined, lang=check_lang) if mode != "link" else []
    google_s = google_search(combined, lang=check_lang) if mode != "link" else []
    
    score = int(gem.get("score", 50))
    verdict = gem.get("verdict", "uncertain")
    
    if verdict == "true":
        score += 10
    elif verdict == "false":
        score -= 20
    
    if google_fc:
        score += 5
    
    if google_s:
        score += 3
    
    domain_info = {}
    if link:
        domain = urlparse(link).netloc
        spam = check_spamhaus(domain)
        safe = check_safe_browsing(link)
        domain_info = {"spamhaus": spam, "safe_browsing": safe}
    
    score = max(0, min(score, 100))
    score = round(score / 20) * 20
    
    try:
        with get_db() as conn:
            cur = conn.cursor()
            query_hash = hash_text(text or combined[:200])
            url_hash = hash_text(link) if link else None
            
            sources_json = json.dumps(gem.get('sources', []))
            
            cur.execute('''
                INSERT INTO checks (query_hash, url_hash, score, verdict, explanation, sources, lang, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ''', (query_hash, url_hash, score, gem.get('verdict', 'uncertain'), 
                gem.get('explanation', ''), sources_json, check_lang, datetime.now()))
            
            conn.commit()
            print(f"‚úÖ Stats saved: score={score}, lang={check_lang}")
    except Exception as e:
        print(f"‚ùå Stats: {e}")

    
    result = {
        "mode": mode,
        "original_text": text,
        "processed_text": combined,
        "article_date": article_date,
        "gemini": gem,
        "google_factcheck": google_fc,
        "google_search": google_s,
        "domain_check": domain_info,
        "score": score
    }
    
    return jsonify(result)

@app.route('/sitemap.xml')
def sitemap():
    return send_from_directory('.', 'sitemap.xml')

@app.route('/robots.txt')
def robots_txt():
    content = """User-agent: *
Allow: /

Sitemap: https://factoryx.com.ua/sitemap.xml"""
    return Response(content, mimetype='text/plain')


if __name__ == "__main__":
    init_db()
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=False)
