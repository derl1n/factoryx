import os
import json
import requests
import psycopg2
from psycopg2.extras import RealDictCursor
import socket
from flask import Flask, request, jsonify, render_template
from urllib.parse import urlparse
from dotenv import load_dotenv
from langdetect import detect
from googletrans import Translator
import dns.resolver
from bs4 import BeautifulSoup
import re
from datetime import datetime
import hashlib

load_dotenv()

# ==========================================================
# DATABASE CONFIGURATION - –ó–ú–Ü–ù–ï–ù–û –î–õ–Ø RENDER
# ==========================================================
DATABASE_URL = os.getenv("DATABASE_URL")  # Render –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–æ–¥–∞—î —Ü–µ

GOOGLE_API_KEY = os.getenv("GOOGLE_FACTCHECK_KEY")
SAFE_BROWSING_KEY = os.getenv("GOOGLE_SAFE_BROWSING_KEY")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
PERPLEXITY_KEY = os.getenv("PERPLEXITY_API_KEY")
SEARCH_KEY = os.getenv("GOOGLE_SEARCH_KEY")
SEARCH_CX = os.getenv("GOOGLE_SEARCH_CX")
SIGHTENGINE_USER = os.getenv("SIGHTENGINE_USER")
SIGHTENGINE_SECRET = os.getenv("SIGHTENGINE_SECRET")

app = Flask(__name__, template_folder="templates", static_folder="static")
translator = Translator()

# ==========================================================
# DATABASE FUNCTIONS - –ó–ú–Ü–ù–ï–ù–û –î–õ–Ø POSTGRESQL
# ==========================================================
def get_db():
    """–ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ PostgreSQL"""
    conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
    return conn

def init_db():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö PostgreSQL"""
    try:
        with get_db() as conn:
            with conn.cursor() as cur:
                # –ß–∏—Ç–∞—î–º–æ SQL –∑ init_db.sql
                with open("init_db.sql", "r", encoding="utf-8") as f:
                    sql = f.read()
                cur.execute(sql)
            conn.commit()
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ë–î: {e}")

# ==========================================================
# HASHING FUNCTIONS
# ==========================================================
def hash_text(text):
    """–•–µ—à—É—î —Ç–µ–∫—Å—Ç –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SHA-256"""
    if not text:
        return None
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

# ==========================================================
# BLOCKED DOMAINS
# ==========================================================
BLOCKED_DOMAINS = [
    # –†–æ—Å—ñ–π—Å—å–∫—ñ —Ç–∞ –±—ñ–ª–æ—Ä—É—Å—å–∫—ñ –¥–æ–º–µ–Ω–∏
    '.ru', '.—Ä—Ñ', '.su',
    '.by',
    'kremlin', 'tass.', 'ria.', 'rbc.', 'kommersant.', 'interfax.',
    'lenta.', 'gazeta.', 'russian.rt.', 'sputnik', 'iz.ru',
    'forbes.ru', 'vedomosti.', 'rossiyskaya-gazeta.', 'rg.ru',
    'belta.by', 'sb.by', 'ont.by',
    # –ö–∞–∑–∏–Ω–æ
    'casino', '–∫–∞–∑–∏–Ω–æ', 'bet', 'betting', '—Å—Ç–∞–≤–∫–∏', 'poker', '–ø–æ–∫–µ—Ä',
    'slots', '—Å–ª–æ—Ç—ã', 'jackpot', '–¥–∂–µ–∫–ø–æ—Ç', 'gambling', '–∞–∑–∞—Ä—Ç–Ω—ñ',
    'azino', 'vulkan', '–≤—É–ª–∫–∞–Ω', 'joycasino', 'slot', 'pin-up',
    'pinup', '1xbet', 'fonbet', 'parimatch', 'leon', 'winline',
    'betfair', 'bwin', '888casino', 'slottica', 'riobet', '777',
    # –î–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (18+)
    'porn', '–ø–æ—Ä–Ω–æ', 'xxx', 'sex', '—Å–µ–∫—Å', 'adult', 'xvideos',
    'pornhub', 'xnxx', 'redtube', 'youporn', 'tube8', 'spankwire',
    'keezmovies', 'chaturbate', 'livejasmin', 'bongacams', 'stripchat',
    'nude', '–≥–æ–ª—ñ', 'naked', 'nsfw', 'erotic', 'erotica', 'hentai',
    'cam4', 'myfreecams', 'camsoda', 'onlyfans', 'manyvids'
]

def is_blocked_source(url):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –¥–∂–µ—Ä–µ–ª–æ –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–µ (–†–§/–ë–õ–†/–ö–∞–∑–∏–Ω–æ/18+)"""
    if not url:
        return False
    url_lower = url.lower()
    for blocked in BLOCKED_DOMAINS:
        if blocked in url_lower:
            return True
    return False

def get_block_reason(url):
    """–í–∏–∑–Ω–∞—á–∞—î –ø—Ä–∏—á–∏–Ω—É –±–ª–æ–∫—É–≤–∞–Ω–Ω—è"""
    if not url:
        return None
    url_lower = url.lower()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ä–æ—Å—ñ–π—Å—å–∫—ñ/–±—ñ–ª–æ—Ä—É—Å—å–∫—ñ
    russian_domains = ['.ru', '.—Ä—Ñ', '.su', 'kremlin', 'tass.', 'ria.', 'rbc.',
                      'kommersant.', 'interfax.', 'lenta.', 'gazeta.', 'sputnik']
    belarusian_domains = ['.by', 'belta.by', 'sb.by', 'ont.by']
    
    for domain in russian_domains:
        if domain in url_lower:
            return "russian"
    for domain in belarusian_domains:
        if domain in url_lower:
            return "belarusian"
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∫–∞–∑–∏–Ω–æ
    casino_keywords = ['casino', '–∫–∞–∑–∏–Ω–æ', 'bet', 'betting', '—Å—Ç–∞–≤–∫–∏', 'poker',
                      '–ø–æ–∫–µ—Ä', 'slots', '—Å–ª–æ—Ç—ã', 'gambling', 'azino', 'vulkan',
                      '1xbet', 'fonbet', 'parimatch', '777']
    for keyword in casino_keywords:
        if keyword in url_lower:
            return "casino"
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ 18+
    adult_keywords = ['porn', '–ø–æ—Ä–Ω–æ', 'xxx', 'sex', 'adult', 'pornhub',
                     'xnxx', 'nude', 'naked', 'nsfw', 'erotic', 'onlyfans']
    for keyword in adult_keywords:
        if keyword in url_lower:
            return "adult"
    
    return None

def filter_sources(sources):
    """–§—ñ–ª—å—Ç—Ä—É—î –¥–∂–µ—Ä–µ–ª–∞, –≤–∏–¥–∞–ª—è—é—á–∏ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ"""
    if not sources:
        return []
    
    filtered = []
    for source in sources:
        if isinstance(source, str):
            if not is_blocked_source(source):
                filtered.append(source)
        elif isinstance(source, dict):
            if not is_blocked_source(source.get('link', '')):
                filtered.append(source)
    
    return filtered

# ==========================================================
# API –ü–ï–†–ï–í–Ü–†–ö–ê ADULT/CASINO –ö–û–ù–¢–ï–ù–¢–£
# ==========================================================
def check_adult_content_sightengine(url):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î URL –Ω–∞ –¥–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ Sightengine API"""
    if not SIGHTENGINE_USER or not SIGHTENGINE_SECRET:
        return {"checked": False, "reason": "API not configured"}
    
    try:
        params = {
            'url': url,
            'models': 'nudity-2.1,offensive',
            'api_user': SIGHTENGINE_USER,
            'api_secret': SIGHTENGINE_SECRET
        }
        
        r = requests.get('https://api.sightengine.com/1.0/check.json', params=params, timeout=10)
        if r.status_code != 200:
            return {"checked": False, "reason": "API error"}
        
        data = r.json()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
        nudity = data.get('nudity', {})
        raw_score = nudity.get('raw', 0)
        partial_score = nudity.get('partial', 0)
        
        # –Ø–∫—â–æ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –¥–æ—Ä–æ—Å–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É > 50%
        if raw_score > 0.5 or partial_score > 0.6:
            return {"checked": True, "blocked": True, "type": "adult", "confidence": max(raw_score, partial_score)}
        
        return {"checked": True, "blocked": False}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Sightengine error: {e}")
        return {"checked": False, "reason": str(e)}

def check_gambling_content(url):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î URL –Ω–∞ –∫–∞–∑–∏–Ω–æ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
        r = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.content, "html.parser")
        
        # –®—É–∫–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∫–∞–∑–∏–Ω–æ –≤ —Ç–µ–∫—Å—Ç—ñ
        text_content = soup.get_text().lower()
        gambling_keywords = [
            'casino', '–∫–∞–∑–∏–Ω–æ', 'poker', '–ø–æ–∫–µ—Ä', 'slots', '—Å–ª–æ—Ç–∏',
            'jackpot', '–¥–∂–µ–∫–ø–æ—Ç', 'roulette', '—Ä—É–ª–µ—Ç–∫–∞', 'blackjack',
            '—Å—Ç–∞–≤–∫–∏', 'betting', 'gambling', '–∞–∑–∞—Ä—Ç–Ω—ñ —ñ–≥—Ä–∏',
            '–±–æ–Ω—É—Å –¥–µ–ø–æ–∑–∏—Ç', 'bonus deposit', 'free spins'
        ]
        
        # –†–∞—Ö—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–±—ñ–≥—ñ–≤
        matches = sum(1 for keyword in gambling_keywords if keyword in text_content)
        
        # –Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –±—ñ–ª—å—à–µ 3 –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ - –π–º–æ–≤—ñ—Ä–Ω–æ –∫–∞–∑–∏–Ω–æ
        if matches >= 3:
            return {"checked": True, "blocked": True, "type": "casino", "matches": matches}
        
        return {"checked": True, "blocked": False}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Gambling check error: {e}")
        return {"checked": False, "reason": str(e)}

def check_safe_browsing_extended(url):
    """–†–æ–∑—à–∏—Ä–µ–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ Google Safe Browsing"""
    try:
        api = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={SAFE_BROWSING_KEY}"
        payload = {
            "client": {"clientId": "factoryx", "clientVersion": "1.0"},
            "threatInfo": {
                "threatTypes": [
                    "MALWARE",
                    "SOCIAL_ENGINEERING",
                    "UNWANTED_SOFTWARE",
                    "POTENTIALLY_HARMFUL_APPLICATION"
                ],
                "platformTypes": ["ANY_PLATFORM"],
                "threatEntryTypes": ["URL"],
                "threatEntries": [{"url": url}]
            }
        }
        
        res = requests.post(api, json=payload, timeout=10)
        jd = res.json()
        
        if jd.get("matches"):
            return {"safe": False, "threat_types": [m.get("threatType") for m in jd.get("matches", [])]}
        
        return {"safe": True}
        
    except Exception as e:
        print(f"‚ö†Ô∏è Safe Browsing error: {e}")
        return {"safe": True}

# ==========================================================
# LANGUAGE DETECTION + TRANSLATION
# ==========================================================
def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

def translate_text(text, target="uk"):
    try:
        return translator.translate(text, dest=target).text
    except:
        return text

@app.route("/translate", methods=["POST"])
def translate_api():
    data = request.json
    text = data.get("text", "")
    target = data.get("target", "uk")
    
    if not text:
        return jsonify({"translated": ""})
    
    return jsonify({"translated": translate_text(text, target)})

# ==========================================================
# QUESTION DETECTION
# ==========================================================
def is_question(text):
    clean = text.strip().lower()
    if clean.endswith("?"):
        return True
    
    q_words = [
        "—Ö—Ç–æ", "—â–æ", "–∫–æ–ª–∏", "–¥–µ", "—á–æ–º—É", "—è–∫", "—Å–∫—ñ–ª—å–∫–∏", "—á–∏",
        "who", "what", "where", "when", "why", "how", "which",
        "–∫—Ç–æ", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∫–∞–∫"
    ]
    
    parts = re.findall(r'\b\w+\b', clean)
    if parts and parts[0] in q_words:
        return True
    
    return False

# ==========================================================
# SUBJECTIVE DETECTION
# ==========================================================
def is_subjective(text):
    subjective_words = [
        "–∫—Ä—É—Ç–∏–π", "–ø–æ–≥–∞–Ω–∏–π", "–∂–∞—Ö–ª–∏–≤–∏–π", "–¥–æ–±—Ä–∏–π", "–≥–∞—Ä–Ω–∏–π",
        "—è –¥—É–º–∞—é", "–º–µ–Ω—ñ –∑–¥–∞—î—Ç—å—Å—è", "–≤–≤–∞–∂–∞—é", "–Ω–∞ –º—ñ–π –ø–æ–≥–ª—è–¥",
        "–∫—Ä–∞—Å–∏–≤–∏–π", "–æ–≥–∏–¥–Ω–∏–π"
    ]
    
    t = text.lower()
    return any(w in t for w in subjective_words)

# ==========================================================
# GIBBERISH DETECTION
# ==========================================================
def is_gibberish(text):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ç–µ–∫—Å—Ç —î –±—ñ–ª—ñ–±–µ—Ä–¥–æ—é"""
    if not text or len(text.strip()) < 5:
        return True
    
    text_no_spaces = text.replace(' ', '').replace('\n', '')
    if len(text_no_spaces) < 5:
        return True
    
    words = re.findall(r'[a-zA-Z–∞-—è–ê-–Ø—ñ—ó—î“ë–Ü–á–Ñ“ê]{2,}', text)
    if len(words) >= 3:
        return False
    
    if re.search(r'(.)\1{5,}', text):
        return True
    
    if len(text) < 20:
        keyboard_patterns = ['qwerty', 'asdfgh', 'zxcvbn', '–π—Ü—É–∫–µ–Ω', '—Ñ—ã–≤–∞–ø', '—è—á—Å–º–∏—Ç']
        text_lower = text.lower()
        if any(pattern in text_lower for pattern in keyboard_patterns):
            return True
    
    if len(text_no_spaces) < 30:
        vowels = 'aeiou–∞–µ—î–∏—ñ—ó–æ—É—é—è'
        has_vowels = any(char.lower() in vowels for char in text)
        if not has_vowels:
            return True
    
    return False

# ==========================================================
# EXTRACT ARTICLE DATE
# ==========================================================
def extract_article_date(soup, url):
    """–í–∏—Ç—è–≥—É—î –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó —Å—Ç–∞—Ç—Ç—ñ –∑ HTML"""
    try:
        meta_dates = [
            soup.find("meta", property="article:published_time"),
            soup.find("meta", {"name": "publish-date"}),
            soup.find("meta", {"name": "date"}),
            soup.find("time")
        ]
        
        for meta in meta_dates:
            if meta:
                date_str = meta.get("content") or meta.get("datetime") or meta.get_text()
                if date_str:
                    try:
                        date_obj = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                        return date_obj.strftime("%Y-%m-%d")
                    except:
                        pass
        
        url_date = re.search(r'(\d{4})[/-](\d{2})[/-](\d{2})', url)
        if url_date:
            year, month, day = url_date.groups()
            return f"{year}-{month}-{day}"
        
        return None
        
    except:
        return None

# ==========================================================
# CLEAN CITATIONS
# ==========================================================
def clean_citations(text):
    """–í–∏–¥–∞–ª—è—î —Ü–∏—Ç—É–≤–∞–Ω–Ω—è —Ç–∏–ø—É [1], [2], [3]"""
    if not text:
        return text
    
    cleaned = re.sub(r'\[\d+\]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

# ==========================================================
# GOOGLE FACTCHECK
# ==========================================================
def google_factcheck(query):
    try:
        url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={GOOGLE_API_KEY}"
        r = requests.get(url, timeout=10)
        claims = r.json().get("claims", [])
        
        filtered_claims = []
        for claim in claims:
            if 'claimReview' in claim:
                has_blocked = False
                for review in claim['claimReview']:
                    if is_blocked_source(review.get('url', '')):
                        has_blocked = True
                        break
                
                if not has_blocked:
                    filtered_claims.append(claim)
            else:
                filtered_claims.append(claim)
        
        return filtered_claims
        
    except:
        return []

# ==========================================================
# GOOGLE SEARCH
# ==========================================================
def google_search(query):
    try:
        url = "https://www.googleapis.com/customsearch/v1"
        res = requests.get(url, params={
            "key": SEARCH_KEY,
            "cx": SEARCH_CX,
            "q": query
        }, timeout=10)
        
        items = res.json().get("items", [])
        results = [
            {
                "title": i.get("title"),
                "snippet": i.get("snippet"),
                "link": i.get("link")
            }
            for i in items[:10]
        ]
        
        filtered = filter_sources(results)
        return filtered[:5]
        
    except:
        return []

# ==========================================================
# PERPLEXITY CHECK
# ==========================================================
def perplexity_check(text, article_date=None):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ Perplexity Sonar API"""
    try:
        MAX_LENGTH = 1500
        if len(text) > MAX_LENGTH:
            text = text[:MAX_LENGTH] + "..."
        
        API = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_KEY}",
            "Content-Type": "application/json"
        }
        
        date_instruction = ""
        if article_date:
            date_instruction = f"\n‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û: –¶—è —Å—Ç–∞—Ç—Ç—è –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∞ {article_date}. –ü–µ—Ä–µ–≤—ñ—Ä—è–π —Ñ–∞–∫—Ç–∏ –Ω–∞ –º–æ–º–µ–Ω—Ç –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó."
        
        source_instruction = (
            "\nüìå –ü–†–Ü–û–†–ò–¢–ï–¢ –î–ñ–ï–†–ï–õ (–≤—ñ–¥ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö):\n"
            "1Ô∏è‚É£ –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ: Suspilne, Ukrainska Pravda, –£–ù–Ü–ê–ù, Kyiv Independent\n"
            "2Ô∏è‚É£ –ó–∞—Ö—ñ–¥–Ω—ñ –∞–≥–µ–Ω—Ü—ñ—ó: Reuters, AP, BBC, AFP, CNN, The Guardian\n"
            "3Ô∏è‚É£ –ú—ñ–∂–Ω–∞—Ä–æ–¥–Ω—ñ: Wikipedia (–∞–Ω–≥–ª—ñ–π—Å—å–∫–∞), DW, Euronews\n"
            "üö´ –ó–ê–ë–û–†–û–ù–ï–ù–û: .ru, .—Ä—Ñ, .su, .by –¥–æ–º–µ–Ω–∏, —Ä–æ—Å—ñ–π—Å—å–∫—ñ –ó–ú–Ü, –∫–∞–∑–∏–Ω–æ, –¥–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç!\n"
            "‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û: –Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç—ñ–ª—å–∫–∏ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ - —à—É–∫–∞–π –∑–∞—Ö—ñ–¥–Ω—ñ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∏ –∞–±–æ –ø–∏—à–∏ 'Insufficient reliable sources'"
        )
        
        payload = {
            "model": "sonar",
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ —Ñ–∞–∫—Ç—á–µ–∫—ñ–Ω–≥—É. –®—É–∫–∞–π –∞–∫—Ç—É–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ. "
                        "–ü–æ–≤–µ—Ä–Ω–∏ JSON: {\"score\": 0-100, \"verdict\": \"true/false/uncertain\", \"explanation\": \"1-2_—Ä–µ—á–µ–Ω–Ω—è\"}. "
                        "–ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ü–∏—Ç—É–≤–∞–Ω–Ω—è [1], [2]! "
                        "–ù–ï –∑–≥–∞–¥—É–π —É –ø–æ—è—Å–Ω–µ–Ω–Ω—ñ –ø—Ä–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞!"
                        f"{date_instruction}"
                        f"{source_instruction}"
                    )
                },
                {
                    "role": "user",
                    "content": f"–ü–µ—Ä–µ–≤—ñ—Ä —Ü–µ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è: {text}"
                }
            ],
            "temperature": 0.1,
            "max_tokens": 400,
            "return_citations": True
        }
        
        r = requests.post(API, json=payload, headers=headers, timeout=30)
        
        if r.status_code != 200:
            print(f"‚ùå Perplexity {r.status_code}")
            return {"error": f"Perplexity API –ø–æ–º–∏–ª–∫–∞ (–∫–æ–¥ {r.status_code})"}
        
        data = r.json()
        content = data["choices"][0]["message"]["content"]
        citations = data.get("citations", [])
        filtered_citations = filter_sources(citations)
        
        print(f"üìä –î–∂–µ—Ä–µ–ª: {len(citations)} ‚Üí –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(filtered_citations)}")
        
        json_match = re.search(r'\{[^{}]*"score"[^{}]*"verdict"[^{}]*"explanation"[^{}]*\}', content, re.DOTALL)
        
        if json_match:
            try:
                result = json.loads(json_match.group())
                
                if not isinstance(result.get("score"), (int, float)):
                    result["score"] = 50
                
                if result.get("verdict") not in ["true", "false", "uncertain"]:
                    result["verdict"] = "uncertain"
                
                explanation = clean_citations(result.get("explanation", ""))
                explanation = re.sub(r'(\.|^)[^.]*(\.\s*ru|\s—Ä—É –¥–æ–º–µ–Ω|—Ä–æ—Å—ñ–π—Å—å–∫[—ñ–∏][–π—Ö]?\s+(–¥–∂–µ—Ä–µ–ª|–ó–ú–Ü|—Å–∞–π—Ç)|–∑–∞–±–æ—Ä–æ–Ω–µ–Ω[—ñ–∏]|–∫–∞–∑–∏–Ω–æ|casino|–¥–æ—Ä–æ—Å–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç|adult content).*?\.', '.', explanation, flags=re.IGNORECASE)
                explanation = re.sub(r'^\s*\.+\s*', '', explanation).strip()
                
                sentences = re.split(r'[.!?]+', explanation)
                sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
                
                if sentences:
                    result["explanation"] = ". ".join(sentences[:2]) + "."
                else:
                    if len(filtered_citations) == 0:
                        result["explanation"] = "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞–¥—ñ–π–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ü—å–æ–≥–æ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è."
                    else:
                        result["explanation"] = "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞ –∫—ñ–ª—å–∫–æ–º–∞ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏."
                
                result["sources"] = filtered_citations[:5]
                
                print(f"‚úÖ Perplexity: score={result['score']}, filtered_sources={len(result['sources'])}")
                
                return result
                
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON decode error: {e}")
                pass
        
        return {
            "score": 50,
            "verdict": "uncertain",
            "explanation": "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏." if len(filtered_citations) == 0 else "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞ –∫—ñ–ª—å–∫–æ–º–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏.",
            "sources": filtered_citations[:5]
        }
        
    except requests.exceptions.Timeout:
        return {"error": "–¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Ç—É"}
    except Exception as e:
        print(f"‚ùå Perplexity: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}

# ==========================================================
# GEMINI CHECK (BACKUP)
# ==========================================================
def gemini_check(text, long=False):
    MAX_LENGTH = 2000 if long else 1000
    if len(text) > MAX_LENGTH:
        text = text[:MAX_LENGTH] + "..."
    
    instruction = (
        "–ü–µ—Ä–µ–≤—ñ—Ä —Ñ–∞–∫—Ç —ñ –ø–æ–≤–µ—Ä–Ω–∏ JSON: {\"score\":0-100, \"verdict\":\"true/false/uncertain\", \"explanation\":\"1-2 —Ä–µ—á–µ–Ω–Ω—è\"}. "
        "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ –∑–∞—Ö—ñ–¥–Ω—ñ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ (BBC, Reuters, AP, Suspilne). "
        "üö´ –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ä–æ—Å—ñ–π—Å—å–∫—ñ (.ru, .—Ä—Ñ), –±—ñ–ª–æ—Ä—É—Å—å–∫—ñ (.by), –∫–∞–∑–∏–Ω–æ —Ç–∞ –¥–æ—Ä–æ—Å–ª—ñ —Å–∞–π—Ç–∏!"
    )
    
    payload = {
        "contents": [{"parts": [{"text": instruction + "\n–§–∞–∫—Ç: " + text}]}],
        "generationConfig": {"temperature": 0.2, "maxOutputTokens": 512}
    }
    
    API = f"https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash-exp:generateContent?key={GEMINI_KEY}"
    
    try:
        r = requests.post(API, json=payload, timeout=20)
        
        if r.status_code != 200:
            return {"error": f"Gemini API –ø–æ–º–∏–ª–∫–∞"}
        
        raw = r.json()
        
        if "candidates" not in raw or not raw["candidates"]:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏", "sources": []}
        
        candidate = raw["candidates"][0]
        
        if "content" not in candidate:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ü–æ–º–∏–ª–∫–∞ AI", "sources": []}
        
        out = candidate["content"]["parts"][0]["text"]
        cleaned = out.replace("```json", "").replace("```", "").strip()
        
        try:
            data = json.loads(cleaned)
            data["sources"] = []
            return data
        except:
            return {"score": 50, "verdict": "uncertain", "explanation": "–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏", "sources": []}
            
    except Exception as e:
        print(f"‚ùå Gemini: {e}")
        return {"error": str(e)}

# ==========================================================
# DOMAIN CHECK
# ==========================================================
def check_spamhaus(domain):
    try:
        q = ".".join(reversed(domain.split("."))) + ".zen.spamhaus.org"
        dns.resolver.resolve(q, "A")
        return {"listed": True}
    except dns.resolver.NXDOMAIN:
        return {"listed": False}
    except:
        return {"listed": False}

def check_safe_browsing(url):
    try:
        api = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={SAFE_BROWSING_KEY}"
        payload = {
            "client": {"clientId": "factoryx", "clientVersion": "1.0"},
            "threatInfo": {
                "threatTypes": ["MALWARE", "SOCIAL_ENGINEERING"],
                "platformTypes": ["ANY_PLATFORM"],
                "threatEntryTypes": ["URL"],
                "threatEntries": [{"url": url}]
            }
        }
        
        res = requests.post(api, json=payload, timeout=10)
        jd = res.json()
        
        return {"safe": not bool(jd.get("matches"))}
        
    except:
        return {"safe": True}

# ==========================================================
# ROUTES
# ==========================================================
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/stats", methods=["GET"])
def get_stats():
    try:
        with get_db() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) as count FROM checks")
                total = cur.fetchone()['count']
                
                cur.execute(
                    "SELECT COUNT(*) as count FROM checks WHERE DATE(created_at) = CURRENT_DATE"
                )
                today = cur.fetchone()['count']
                
                cur.execute(
                    "SELECT COUNT(*) as count FROM checks WHERE created_at >= NOW() - INTERVAL '7 days'"
                )
                week = cur.fetchone()['count']
        
        return jsonify({
            "total_checks": total,
            "today": today,
            "week": week,
            "status": "ok"
        })
        
    except Exception as e:
        print(f"‚ùå –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {e}")
        return jsonify({"error": "Database error"}), 500

# ==========================================================
# MAIN FACT CHECK
# ==========================================================
@app.route("/check", methods=["POST"])
def check_fact():
    data = request.json
    text = (data.get("text") or "").strip()
    link = (data.get("link") or "").strip()
    lang = data.get("lang", "uk")
    
    if text and link:
        mode = "both"
    elif text:
        mode = "text"
    else:
        mode = "link"
    
    error_messages = {
        "uk": {
            "no_text": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç",
            "text_short": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç (–º—ñ–Ω—ñ–º—É–º 10 —Å–∏–º–≤–æ–ª—ñ–≤ —Ç–∞ 2 —Å–ª–æ–≤–∞)",
            "no_link": "‚ùå –í–≤–µ–¥—ñ—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è",
            "question": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è, –∞ –Ω–µ –ø–∏—Ç–∞–Ω–Ω—è",
            "subjective": "‚ùå –¶–µ —Å—É–± º—î–∫—Ç–∏–≤–Ω–µ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è",
            "gibberish": "‚ùå –í–≤–µ–¥—ñ—Ç—å —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏",
            "domain_not_exist": "‚ùå –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–µ –ø—Ä–∞—Ü—é—î - –¥–æ–º–µ–Ω –Ω–µ —ñ—Å–Ω—É—î –∞–±–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π",
            "page_load_failed": "‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∞–±–æ –Ω–∞–¥—ñ—à–ª—ñ—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É",
            "no_text_extracted": "‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è. –ù–∞–¥—ñ—à–ª—ñ—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É",
            "phishing": "üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ï –ü–û–°–ò–õ–ê–ù–ù–Ø! Google Safe Browsing –≤–∏—è–≤–∏–≤ (—Ñ—ñ—à–∏–Ω–≥/—à–∫—ñ–¥–ª–∏–≤–µ –ü–ó)",
            "spam": "üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ò–ô –î–û–ú–ï–ù! Spamhaus –ø–æ–∑–Ω–∞—á–∏–≤ —Ü–µ–π –¥–æ–º–µ–Ω —è–∫ —à–∫—ñ–¥–ª–∏–≤–∏–π",
            "blocked_russian": "üö´ –†–æ—Å—ñ–π—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_belarusian": "üö´ –ë—ñ–ª–æ—Ä—É—Å—å–∫—ñ –¥–∂–µ—Ä–µ–ª–∞ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_casino": "üö´ –°–∞–π—Ç–∏ –∫–∞–∑–∏–Ω–æ —Ç–∞ –∞–∑–∞—Ä—Ç–Ω–∏—Ö —ñ–≥–æ—Ä –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_adult": "üö´ –°–∞–π—Ç–∏ –¥–æ—Ä–æ—Å–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (18+) –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è",
            "blocked_casino_detected": "üö´ –í–∏—è–≤–ª–µ–Ω–æ —Å–∞–π—Ç –∫–∞–∑–∏–Ω–æ –∞–±–æ –∞–∑–∞—Ä—Ç–Ω–∏—Ö —ñ–≥–æ—Ä",
            "blocked_adult_detected": "üö´ –í–∏—è–≤–ª–µ–Ω–æ —Å–∞–π—Ç –¥–æ—Ä–æ—Å–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É 18+"
        },
        "en": {
            "no_text": "‚ùå Enter text",
            "text_short": "‚ùå Enter text (minimum 10 characters and 2 words)",
            "no_link": "‚ùå Enter a link",
            "question": "‚ùå Enter a statement, not a question",
            "subjective": "‚ùå This is subjective",
            "gibberish": "‚ùå Enter valid text",
            "domain_not_exist": "‚ùå Link doesn't work - domain doesn't exist or unavailable",
            "page_load_failed": "‚ùå Failed to load page. Check the link or send text manually",
            "no_text_extracted": "‚ùå Failed to extract text from link. Send text manually",
            "phishing": "üö® DANGEROUS LINK! Google Safe Browsing detected (phishing/malware)",
            "spam": "üö® DANGEROUS DOMAIN! Spamhaus blacklisted this domain",
            "blocked_russian": "üö´ Russian sources are not supported",
            "blocked_belarusian": "üö´ Belarusian sources are not supported",
            "blocked_casino": "üö´ Casino and gambling sites are not supported",
            "blocked_adult": "üö´ Adult content sites (18+) are not supported",
            "blocked_casino_detected": "üö´ Casino/gambling site detected",
            "blocked_adult_detected": "üö´ Adult content site (18+) detected"
        }
    }
    
    errors = error_messages.get(lang, error_messages["uk"])
    
    # –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê –ó–ê–ë–û–†–û–ù–ï–ù–Ü –°–ê–ô–¢–ò (–ö–õ–Æ–ß–û–í–Ü –°–õ–û–í–ê)
    if link and is_blocked_source(link):
        block_reason = get_block_reason(link)
        
        if block_reason == "russian":
            return jsonify({"error": errors["blocked_russian"]}), 400
        elif block_reason == "belarusian":
            return jsonify({"error": errors["blocked_belarusian"]}), 400
        elif block_reason == "casino":
            return jsonify({"error": errors["blocked_casino"]}), 400
        elif block_reason == "adult":
            return jsonify({"error": errors["blocked_adult"]}), 400
        else:
            return jsonify({"error": errors["blocked_russian"]}), 400
    
    if mode == "text":
        if not text:
            return jsonify({"error": errors["no_text"]}), 400
        
        words = text.split()
        if len(text) < 10 or len(words) < 2:
            return jsonify({"error": errors["text_short"]}), 400
    
    if mode == "link":
        if not link:
            return jsonify({"error": errors["no_link"]}), 400
    
    if mode == "both":
        if not link:
            return jsonify({"error": errors["no_link"]}), 400
    
    if text and is_question(text):
        return jsonify({"error": errors["question"]}), 400
    
    if text and is_subjective(text):
        return jsonify({"error": errors["subjective"]}), 400
    
    if text and is_gibberish(text):
        return jsonify({"error": errors["gibberish"]}), 400
    
    # –ü–ï–†–ï–í–Ü–†–ö–ê –ü–û–°–ò–õ–ê–ù–ù–Ø –ß–ï–†–ï–ó API
    if link and link.startswith("http"):
        domain = urlparse(link).netloc
        print(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–µ–∑–ø–µ–∫–∏: {domain}")
        
        try:
            socket.gethostbyname(domain)
            print(f"  ‚úÖ –î–æ–º–µ–Ω —ñ—Å–Ω—É—î")
        except socket.gaierror:
            print(f"  ‚ùå –î–æ–º–µ–Ω –ù–ï —ñ—Å–Ω—É—î!")
            return jsonify({"error": errors["domain_not_exist"]}), 400
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ñ—ñ—à–∏–Ω–≥
        safe_check = check_safe_browsing(link)
        spam_check = check_spamhaus(domain)
        
        print(f"  Safe Browsing: {safe_check}")
        print(f"  Spamhaus: {spam_check}")
        
        if not safe_check["safe"]:
            print("üö® –ù–ï–ë–ï–ó–ü–ï–ß–ù–ï –ü–û–°–ò–õ–ê–ù–ù–Ø –í–ò–Ø–í–õ–ï–ù–û!")
            return jsonify({"error": errors["phishing"]}), 400
        
        if spam_check["listed"]:
            print("üö® –î–û–ú–ï–ù –í –°–ü–ê–ú-–°–ü–ò–°–ö–£!")
            return jsonify({"error": errors["spam"]}), 400
        
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê ADULT –ö–û–ù–¢–ï–ù–¢–£ (—è–∫—â–æ —î API)
        if SIGHTENGINE_USER and SIGHTENGINE_SECRET:
            adult_check = check_adult_content_sightengine(link)
            print(f"  üîû Adult Check: {adult_check}")
            
            if adult_check.get("blocked"):
                print("üö® –í–ò–Ø–í–õ–ï–ù–û –î–û–†–û–°–õ–ò–ô –ö–û–ù–¢–ï–ù–¢!")
                return jsonify({"error": errors["blocked_adult_detected"]}), 400
        
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ö–ê–ó–ò–ù–û (—á–µ—Ä–µ–∑ –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É)
        gambling_check = check_gambling_content(link)
        print(f"  üé∞ Gambling Check: {gambling_check}")
        
        if gambling_check.get("blocked"):
            print("üö® –í–ò–Ø–í–õ–ï–ù–û –°–ê–ô–¢ –ö–ê–ó–ò–ù–û!")
            return jsonify({"error": errors["blocked_casino_detected"]}), 400
    
    page_text = ""
    article_date = None
    
    if link and link.startswith("http"):
        try:
            r = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(r.content, "html.parser")
            
            article_date = extract_article_date(soup, link)
            if article_date:
                print(f"üìÖ –î–∞—Ç–∞ —Å—Ç–∞—Ç—Ç—ñ: {article_date}")
            
            for bad in soup(["script", "style", "header", "footer", "nav"]):
                bad.decompose()
            
            blocks = [
                t.get_text().strip()
                for t in soup.find_all(["p", "article", "section"])
                if len(t.get_text().strip()) > 25
            ]
            
            page_text = " ".join(blocks[:80])
            
            if not text and not page_text:
                print("‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π page_text")
                return jsonify({"error": errors["no_text_extracted"]}), 400
            
            if not text:
                text = page_text[:500]
                
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            if not text:
                return jsonify({"error": errors["page_load_failed"]}), 400
    
    combined = f"{text} {page_text}".strip()
    detected = detect_language(combined)
    
    if detected == "uk":
        query = combined
    elif detected == "ru":
        query = translate_text(combined, "uk")
    elif detected == "en":
        query = combined
    else:
        query = translate_text(combined, "en")
    
    is_long = len(query) > 900
    gem = None
    
    print(f"üîç Perplexity: {query[:100]}...")
    gem = perplexity_check(query, article_date=article_date)
    
    if "error" in gem and GEMINI_KEY:
        print(f"‚ö†Ô∏è Perplexity failed, Gemini backup...")
        gem = gemini_check(query, long=is_long)
    
    if "error" in gem:
        return jsonify({"error": gem["error"]}), 500
    
    google_fc = google_factcheck(query) if mode != "link" else []
    google_s = google_search(query) if mode != "link" else []
    
    score = int(gem.get("score", 50))
    verdict = gem.get("verdict", "uncertain")
    
    if verdict == "true":
        score += 10
    elif verdict == "false":
        score -= 20
    
    if google_fc:
        score += 5
    
    if google_s:
        score += 3
    
    domain_info = {}
    if link:
        domain = urlparse(link).netloc
        spam = check_spamhaus(domain)
        safe = check_safe_browsing(link)
        domain_info = {"spamhaus": spam, "safe_browsing": safe}
    
    score = max(0, min(score, 100))
    score = round(score / 20) * 20
    
    try:
        with get_db() as conn:
            with conn.cursor() as cur:
                query_hash = hash_text(text or query[:200])
                url_hash = hash_text(link) if link else None
                
                cur.execute('''
                    INSERT INTO checks (query_hash, url_hash, score, created_at)
                    VALUES (%s, %s, %s, %s)
                ''', (query_hash, url_hash, score, datetime.now()))
            
            conn.commit()
        
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞: score={score}")
        
    except Exception as e:
        print(f"‚ùå –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {e}")
    
    result = {
        "mode": mode,
        "original_text": text,
        "processed_text": query,
        "article_date": article_date,
        "gemini": gem,
        "google_factcheck": google_fc,
        "google_search": google_s,
        "domain_check": domain_info,
        "score": score
    }
    
    return jsonify(result)

if __name__ == "__main__":
    init_db()
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=False)
